\section{Introduction}
\label{sec:intro}

% References:
% An Analysis of Assessor Behavior in Crowdsourced Preference Judgments
% Dongqing Zhu and Ben Carterette {zhu2010analysis}

% NEW INTRO
How can we reliably obtain information from humans, given that the humans 
themselves are unreliable, and might even have incentives to mislead us?
This question is important, for instance, if we wish to curate a large 
dataset via crowdsourcing --- many of the workers in the crowd do not produce 
high-quality data, but they are incentivized to fool us into thinking that 
it is high-quality in order to get paid. This raises the question: given a 
core of reliable workers, where the rest of the workers are unreliable and 
potentially adversarial, can we obtain almost all of the data from the 
reliable workers while allowing in almost none of the data from the 
unreliable workers?

In this work, we answer this question in the affirmative, under surprisingly 
weak assumptions that improve substantially upon previous results. 
In particular:
\begin{itemize}
\item We do not assume that there is a ``gold set'' or other cheap way to judge 
      worker performance (instead, we rely on a small number of our own post hoc judgments).
\item We do not assume that the majority of workers are reliable.
\item We do not assume that the unreliable workers conform to any statistical 
      model; they could behave fully adversarially, in collusion with each other 
      and with full knowledge of how the reliable workers behave.
\item We do not assume that worker judgments match our own, but only that they are 
      ``approximately monotonic'' in our judgments, in a sense that will be 
      formalized later.
\end{itemize}
For concreteness, let us consider a simple formalization of the crowdsourcing 
setting in which to describe our algorithm and results (our actual results hold 
in a more general setting). We assume that there are $n$ workers, at least 
$\alpha n$ of whom are reliable in the following sense: (i) $90\%$ of the data 
that they produce is correct, and (ii) when they judge whether someone else's data 
is correct, they are right with probability at least $60\%$ and make independent 
errors. Each worker is assigned to judge a randomly selected output from each 
of $k$ other workers (also chosen at random). In addition, we ourselves judge 
an output from each of $k_0$ workers. The results of this paper imply the 
following:

\begin{theorem}
\label{thm:intro}
In the scenario above, suppose that $k \geq \Omega(1/\alpha^4\epsilon^4)$
and that $k_0 \geq \Omega(\log(1/\alpha\epsilon)/\alpha\epsilon^2)$. Then, with probability 
at least $99\%$, we will select $\alpha n$ workers, whose data is correct on 
average with probability at least $0.9-\epsilon$.
\end{theorem}
In other words, compared to the situation where we know exactly who is reliable, 
we obtain the same amount of data, and its average quality is worse by at most 
$\epsilon$. Amazingly, the amount of work that each worker (and we ourselves) has 
to do does not grow with $n$; it depends only on the fraction $\alpha$ of 
reliable workers and the the desired accuracy $\epsilon$.

Our algorithm only exploits inter-rater agreement among the reliable workers, 
and does rely on the fact that reliable workers also produce reliable data.
Our results therefore hold in a more general setting where we have $n$ raters and 
$m$ items, and $\alpha n$ of the raters are reliable; our goal is to 
recover the \emph{$\beta$-quantile} of the items --- the $\beta m$ items 
with the highest rating according to our own judgment. In the setting above, 
the raters are the workers and the items are the data they produce. Our original 
goal corresponded to recovering the $\alpha$-quantile, i.e. the data 
produced by the $\alpha n$ best workers.

Another scenario that our setting applies to is peer assessment in MOOCs (massive 
online open courses). Here a single class could have thousands of students, 
and to achieve scalability instructors often ask students to grade each others' 
papers \citep{kulkarni2015peer,piech2013tuned}. 
Some student graders will simply not have mastered the material themselves, 
and some enterprising but dishonest students are likely to collude, 
so building grading systems that are robust to large errors as well as 
collusion is crucial. 
Our results apply to this setting as well, and imply that we can estimate the 
$\beta$-quantile after asking each student to grade 
$\Omega(1/\beta\alpha^3\epsilon^4)$ other student papers at random, and 
grading $\Omega(\log(1/\alpha)/\beta\epsilon^2)$ papers ourselves.
Estimating, say, the $0.1$-quantile is 
sensible if we wish to assign A's to the top $10\%$ of the class. 
An important aspect of our results is that they hold even if some graders are 
more lenient than others, as long as the \emph{ranking} produces by the graders 
is the same. The focus on quantiles rather than raw scores is what enables this 
(note that once we estimate the quantiles, we can approximately recover the 
raw scores by evaluating a few items in each quantile).

% Overview of Related Work
% Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems
%  {karger2014budget}
%  uses belief propagation to model worker reliability, assign workers to tasks
% Approval Voting and Incentives in Crowdsourcing
%  {shah2015approval}
% Double or nothing: Multiplicative incentive mechanisms for crowdsourcing
%  {shah2015double}
%  same as above
% Regularized Minimax Conditional Entropy for Crowdsourcing
%  {zhou2015regularized}
%  models worker reliability and task difficulty
% Truth Serums for Massively Crowdsourced Evaluation Tasks
%  {kamble2015truth}
% Eliciting Informative Feedback: The Peer-Prediction Method
%  -- in some sense, defined our problem setting
%  -- very central paper
%  {miller2005eliciting}
%  other related papers: {shnayder2016strong}, {dasgupta2013crowdsourced}

% Evidence for our mechanism being important
% {harmon2004amazon} --- manipulation of book reviews
% {mayzlin2006promotional,white1999chatting} --- manipulation of online bulletin boards
% MOOCs 
%   Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and
%   Daphne Koller. Tuned models of peer assessment in MOOCs. arXiv preprint
%   arXiv:1307.2579, 2013.
% Crowdsourcing
%   Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, 
%   Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. 
%   The Journal of Machine Learning Research, 11:1297â€“1322, 2010.
% Yelp
%   Michael Luca. Reviews, reputation, and revenue: The case of yelp. com. Com
%   (September 16, 2011). Harvard Business School NOM Unit Working Paper, 
%   (12-016), 2011.
% {dellarocas2006strategic} --- general survey
% The Influence Limiter: Provably Manipulation-Resistant Recommender Systems
%  seems very similar to ours, but they assume online feedback of ground truth
%  {resnick2007influence}

% TODO: who to cite?
% Improved Graph Clustering {chen2014improved}
% A Proof Of The Block Model Threshold Conjecture
% Community detection thresholds and the weak ramanujan property.
%  Community detection in sparse networks via Grothendieck_0s inequality
Our technical tools draw on semidefinite programming methods for matrix 
completion, which have been used to study graph clustering as well 
as the stochastic block model \citep{holland1983stochastic,condon2001algorithms}. 
Our setting corresponds to the sparse case where all nodes have constant degree, 
which has recently seen great interest \citep{decelle2011asymptotic,
mossel2012stochastic,mossel2013proof,mossel2013belief,
massoulie2014community,guedon2014community,mossel2015consistency,
chin2015stochastic,abbe2015community,makarychev2015learning}. 
\citet{makarychev2015learning} in particular provide an algorithm that is 
robust to adversarial perturbations, but only if the perturbation has 
size $o(n)$; see also \citet{cai2015robust} for robusness results when 
the node degree is logarithmic.
% NOTE: decelle2011asymptotic make a conjecture that implies 
% 1/alpha^2epsilon^2 is tight even in a very simple case

Several authors have considered semirandom settings for graph clustering, which 
allow for some types of adversarial behavior \citep{feige2000finding,
feige2001heuristics,coja2004coloring,krivelevich2006semirandom,
coja2007solving,makarychev2012approximation,chen2014improved,guedon2014community,
moitra2015robust,agarwal2015multisection}. 
In our setting, these semirandom models would need to assume that the adversaries 
are strictly dominated by the reliable raters (in the sense of having lower 
expected accuracy on every item); this is implausible as it rules out 
most types of strategic behavior.
In removing this assumption, we face a key technical challenge: while previous 
analyses consider errors relative to a ground truth clustering 
\citep[e.g.][]{chen2014improved}, in our setting 
the ground truth only exists for rows of the matrix corresponding to reliable 
raters, while the remaining rows could behave arbitrarily even in the limit 
where all ratings are observed. This necessitates a more careful analysis, 
which has the advantage of clarifying which properties of previous clustering 
models were necessary and which were superfluous.


\paragraph{Overview of the paper.} In Section~\ref{sec:algorithm}, we 
present our algorithm and explain the intuition behind it, as well as why 
some similar algorithms would be less robust to adversaries. In 
Section~\ref{sec:approach}, we present our formal assumptions and explain 
the key ingredients in our proof, with technical details deferred to the 
appendix. In Section~\ref{sec:discussion}, we discuss our results and 
place them in the context of existing work.
