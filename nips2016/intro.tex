\section{Introduction}
\label{sec:intro}

% References:
% An Analysis of Assessor Behavior in Crowdsourced Preference Judgments
% Dongqing Zhu and Ben Carterette {zhu2010analysis}

% NEW INTRO
How can we reliably obtain information from humans, given that the humans 
themselves are unreliable, and might even have incentives to mislead us?
Versions of this question arise in crowdsourcing \citepa{vuurens2011spam}, 
collaborative knowledge generation \citepm{priedhorsky2007creating}, peer grading 
in online classrooms \citepm{piech2013tuned,kulkarni2015peer},  aggregation 
of customer reviews \citepa{harmon2004amazon}, and the generation/curation of large datasets \citepm{deng2009imagenet}. A key challenge is to ensure 
high information quality despite the fact that many people interacting with 
the system may be unreliable or even adversarial.
This is particularly relevant when raters have an incentive to collude and 
cheat as in the setting of peer grading, as well as for reviews on sites like 
Amazon and Yelp, 
where artists and firms are incentivized to manufacture positive reviews 
for their own products and negative reviews for their rivals \citepa{harmon2004amazon,mayzlin2012promotional}.

% talk about gold sets --- say not okay because too constrainint
One approach to ensuring quality is to use \emph{gold sets} --- questions where 
the answer is known, which can be used to assess reliability on unknown questions. 
However, this is overly constraining --- it does not make sense for open-ended 
tasks such as knowledge generation on wikipedia, nor even for crowdsourcing 
tasks such as ``translate this paragraph'' or ``draw an interesting picture'' 
where there are different equally good answers.   This approach may also fail 
in settings, such as %orchestrating an effective 
peer grading in massive online open courses, where 
students might collude to inflate their grades.

In this work, we consider the challenge of using crowdsourced human ratings to accurately and efficiently evaluate a large dataset of content.   In some settings, such as peer grading, the end goal is to obtain the accurate evaluation of each datum; in other settings, such as the curation of a large dataset, accurate evaluations could be leveraged to select a high-quality subset of a larger set of variable-quality (perhaps crowd-generated) data.  

There are several confounding difficulties that arise in extracting accurate evaluations.  First, many raters may be unreliable and give evaluations 
that are uncorrelated with the actual item quality; 
second, some reliable raters might be harsher or more lenient than others; 
third, some items may be harder to evaluate than others 
%(e.g. long vs. short sentences) 
and so error rates could vary from item to item, even among reliable raters; 
finally, some raters may even collude or want to hack the system. % (e.g. one worker agrees to rate another's output highly).
This raises the question: can we obtain 
information from the reliable raters, without knowing who they are a priori?


\iffalse

In this work, we take a different perspective: we make no restrictions on the content that people can produce, and instead focus on \emph{evaluating} this 
content, using human ratings to achieve scalability in this evaluation task. 
For instance, in a crowdsourcing setting where we wanted to translate paragraphs 
from Chinese to English, we would first ask workers to perform the translation, 
and then ask a (potentially) new set of workers to evaluate the quality of 
the translations.   

% our insight --- let humans rate other humans!
% challenges: different scales, different responses to different items
This leads to several challenges. First, many workers may be unreliable; 
second, some reliable raters might be harsher or more lenient than others; 
third, some translations may be harder to evaluate than others 
%(e.g. long vs. short sentences) 
and so rater error rates could vary from item to item; 
finally, some workers may even collude. % (e.g. one worker agrees to rate another's output highly).
Despite these difficulties, though, at least some workers 
actually are reliable. This raises the question: can we obtain 
information from the reliable workers, without knowing who they are a priori?
\fi

In this work, we answer this question in the affirmative, under surprisingly 
weak assumptions:
%\begin{itemize}
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=30pt]
\item We do not assume that the majority of workers are reliable.
\item We do not assume that the unreliable workers conform to any statistical 
      model; they could behave fully adversarially, in collusion with each other 
      and with full knowledge of how the reliable workers behave.
\item We do not assume that the reliable worker ratings match the true ratings, but only that they are 
      ``approximately monotonic'' in the true ratings, in a sense that will be 
      formalized later.
\item We do not assume that there is a ``gold set'' of items with known ratings 
      presented to each user (as an adversary could identify and exploit this). 
      Instead, we rely on a small number of reliable judgments on randomly selected items, 
      obtained after the workers submit their own ratings; 
      in practice, these could be obtained by rating those items oneself.
\end{itemize}
For concreteness, we describe a simple formalization of the crowdsourcing 
setting (our actual results hold in a more general setting). 
We imagine that we are the dataset curator, so that ``us'' and ``ourselves'' refers 
in general to whoever is curating the data.
There are $n$ raters and $m$ items to evaluate, which have an unknown 
quality level in $[0,1]$. At least $\alpha n$ workers are ``reliable'' in that 
their judgments match our own in expectation, and they make independent errors.
We assign each worker to evaluate at most $k$ randomly selected items. 
In addition, we ourselves judge $k_0$ items. Our goal is to 
recover the \emph{$\beta$-quantile}: the set $T^*$ of the $\beta n$ highest-quality items. 
%
Our main result is the following:

\input matrix-figure

\begin{theorem}
\label{thm:intro}
In the setting above, suppose $n = m$, 
$k \geq \Omega(\tfrac{1}{\beta\alpha^3\epsilon^4})$, and 
$k_0 \geq \Omega(\tfrac{\log(2/\alpha\beta)}{\beta\epsilon^2})$. Then, with probability 
at least $99\%$, we can identify $\beta n$ items with average quality at most 
$\epsilon$ worse than $T^*$.
\end{theorem}
Interestingly, the amount of work that each worker (and we ourselves) has 
to do does not grow with $n$; it depends only on the fraction $\alpha$ of 
reliable workers and the desired accuracy $\epsilon$. While the number 
of evaluations $k$ for each worker is likely not optimal, we note that 
the amount of work $k_0$ required of us is close to optimal: 
for $\alpha \le \beta$, it is information theoretically necessary for 
us to evaluate $\Omega(1/\beta\eps^2)$ items, via a reduction to estimating 
noisy coin flips. % \citepa{mannor2004sample}.
%\footnote{This can be shown via a reduction to the task of selecting a coin of bias $1/2+\eps$ from among $1/\beta$ coins of bias $1/2$. \iffalse To see this, consider the setting where $\alpha = \beta$ and imagine that the items are partitioned into $1/\beta$ sets; suppose the workers are also partitioned into $1/\beta$ sets with the reliable workers consisting of one such set.  Suppose the $i$th set of workers behaves as if the $i$th set of items have quality $1/2+\eps$, and the rest of the items have quality $1/2$.  By the symmetry of the setting, the ratings give no information into which set of items actually have quality $1/2+\eps$, hence we must determine this via our own evaluations, which is analogous to the task of selecting the coin of bias $1/2+\eps$ from among $1/\beta$ coins of bias $1/2$, which requires $1/\beta \eps^2$ flips, in expectation.\fi }

Why is it necessary to include some of our own ratings? 
If we did not, and $\alpha < \frac{1}{2}$, then an adversary could create a set of 
dishonest raters that were identical to the reliable raters except with the 
item indices permuted by a random permutation of $\{1,\ldots,m\}$. In this case, 
there is no way to distinguish the honest from the dishonest raters except by 
breaking the symmetry with our own ratings.

Our main result holds in a considerably more general setting where we require a 
weaker form of inter-rater agreement --- for example, our results hold even if 
some of the reliable raters are harsher than others, as long as 
the expected ratings induce approximately the same ranking.
The focus on quantiles rather than raw ratings is what enables this. 
Note that once we estimate the quantiles, we can approximately recover the 
ratings by evaluating a few items in each quantile.

%Another setting that our results apply to is peer grading in massive online open courses. 
%A single class could have thousands of students, 
%and to achieve scalability instructors often ask students to grade each others' 
%papers \citepm{kulkarni2015peer,piech2013tuned}. 
%Some student graders will simply not have mastered the material themselves (and hence may give unreliable grades), 
%and some enterprising but dishonest students are likely to collude. Theorem~\ref{thm:intro} 
%implies that our algorithm is robust to both of these possibilities. 
%It also automatically adjusts for differences in leniency across graders.

% Overview of Related Work
% Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems
%  {karger2014budget}
%  uses belief propagation to model worker reliability, assign workers to tasks
% Approval Voting and Incentives in Crowdsourcing
%  {shah2015approval}
% Double or nothing: Multiplicative incentive mechanisms for crowdsourcing
%  {shah2015double}
%  same as above
% Regularized Minimax Conditional Entropy for Crowdsourcing
%  {zhou2015regularized}
%  models worker reliability and task difficulty
% Truth Serums for Massively Crowdsourced Evaluation Tasks
%  {kamble2015truth}
% Eliciting Informative Feedback: The Peer-Prediction Method
%  -- in some sense, defined our problem setting
%  -- very central paper
%  {miller2005eliciting}
%  other related papers: {shnayder2016strong}, {dasgupta2013crowdsourced}

% Evidence for our mechanism being important
% {harmon2004amazon} --- manipulation of book reviews
% {mayzlin2006promotional,white1999chatting} --- manipulation of online bulletin boards
% MOOCs 
%   Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and
%   Daphne Koller. Tuned models of peer assessment in MOOCs. arXiv preprint
%   arXiv:1307.2579, 2013.
% Crowdsourcing
%   Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, 
%   Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. 
%   The Journal of Machine Learning Research, 11:1297–1322, 2010.
% Yelp
%   Michael Luca. Reviews, reputation, and revenue: The case of yelp. com. Com
%   (September 16, 2011). Harvard Business School NOM Unit Working Paper, 
%   (12-016), 2011.
% {dellarocas2006strategic} --- general survey
% The Influence Limiter: Provably Manipulation-Resistant Recommender Systems
%  seems very similar to ours, but they assume online feedback of ground truth
%  {resnick2007influence}

% TODO: who to cite?
% Improved Graph Clustering {chen2014improved}
% A Proof Of The Block Model Threshold Conjecture
% Community detection thresholds and the weak ramanujan property.
%  Community detection in sparse networks via Grothendieck_0s inequality
Our technical tools draw on semidefinite programming methods for matrix 
completion, which have been used to study graph clustering as well 
as community detection in the stochastic block model \citepm{holland1983stochastic,condon2001algorithms}. 
Our setting corresponds to the sparse case of graphs with constant degree, 
which has recently seen great interest \citepa{decelle2011asymptotic,
mossel2012stochastic,mossel2013proof,mossel2013belief,
massoulie2014community,guedon2014community,mossel2015consistency,
chin2015stochastic,abbe2015community,makarychev2015learning}. 
\citetm{makarychev2015learning} in particular provide an algorithm that is 
robust to adversarial perturbations, but only if the perturbation has 
size $o(n)$; see also \citetm{cai2015robust} for robusness results when 
the degree of the graph is logarithmic.
% NOTE: decelle2011asymptotic make a conjecture that implies 
% 1/alpha^2epsilon^2 is tight even in a very simple case

Several authors have considered semirandom settings for graph clustering, which 
allow for some types of adversarial behavior \citepa{feige2000finding,
feige2001heuristics,coja2004coloring,krivelevich2006semirandom,
coja2007solving,makarychev2012approximation,chen2014improved,guedon2014community,
moitra2015robust,agarwal2015multisection}. 
In our setting, these semirandom models are unsuitable because they rule out 
important types of strategic behavior, such as an adversary rating some items 
accurately to gain credibility.
%would need to assume that the adversaries 
%are strictly dominated by the reliable raters, in the sense of having lower 
%expected accuracy on every item; this is implausible as it rules out 
%most types of strategic behavior.
By allowing arbitrary behavior from the adversary, 
we face a key technical challenge: while previous 
analyses consider errors relative to a ground truth clustering, 
%\citepm[e.g.][]{chen2014improved}, 
in our setting 
the ground truth only exists for rows of the matrix corresponding to reliable 
raters, while the remaining rows could behave arbitrarily even in the limit 
where all ratings are observed. This necessitates a more careful analysis, 
which helps to clarify what properties of a clustering are truly necessary 
for identifying it.


%\paragraph{Overview of the paper.} In Section~\ref{sec:algorithm}, we 
%present our algorithm and explain the intuition behind it.
%In Section~\ref{sec:approach}, we present our formal assumptions and main 
%result. 
%In Sections~\ref{sec:approach-M} and \ref{sec:approach-T}, we explain 
%the key ingredients in our proof.
%In Section~\ref{sec:discussion}, we discuss our results and 
%present open problems.
