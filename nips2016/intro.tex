\section{Introduction}
\label{sec:intro}

% References:
% An Analysis of Assessor Behavior in Crowdsourced Preference Judgments
% Dongqing Zhu and Ben Carterette {zhu2010analysis}

% NEW INTRO
How can we reliably obtain information from humans, given that the humans 
themselves are unreliable, and might even have incentives to mislead us?
This question is important, for instance, if we wish to curate a large 
dataset via crowdsourcing --- many of the workers in the crowd do not produce 
high-quality data, but they are incentivized to fool us into thinking that 
it is high-quality in order to get paid. This raises the question: given a 
core of reliable workers, where the rest of the workers are unreliable and 
potentially adversarial, can we obtain almost all of the data from the 
reliable workers while allowing in almost none of the data from the 
unreliable workers?

In this work, we answer this question in the affirmative, under surprisingly 
weak assumptions that improve substantially upon previous results. 
In particular:
\begin{itemize}
\item We do not assume that there is a ``ground truth'' upon which to judge 
      worker performance (except for a small number of our own post hoc judgments).
\item We do not assume that the majority of workers are reliable.
\item We do not assume that the unreliable workers conform to any statistical 
      model; they could behave fully adversarially, in collusion with each other 
      and with full knowledge of how the reliable workers behave.
\item We do not assume that worker judgments match our own, but only that they are 
      ``approximately monotonic'' in our judgments, in a sense that will be 
      formalized later.
\end{itemize}
For concreteness, let us consider a simple formalization of the crowdsourcing 
setting in which to describe our algorithm and results (our actual results hold 
in a more general setting). We assume that there are $N$ workers, at least 
$\alpha N$ of whom are reliable in the following sense: (i) $90\%$ of the data 
that they produce is correct, and (2) when they judge whether someone else's data 
is correct, they are right with probability at least $60\%$ and make independent 
errors. Each worker is assigned to judge a randomly selected output from each 
of $k$ other workers (also chosen at random). In addition, we ourselves judge 
an output from each of $k'$ workers. The results of this paper imply the 
following:

\begin{theorem}
\label{thm:intro}
In the scenario above, suppose that $k \geq \Omega(1/\alpha^4\epsilon^4)$
and that $k' >= \Omega(\log(1/\alpha)/\alpha\epsilon^2)$. Then, with probability 
at least $90\%$, we will select $\alpha N$ workers, whose data is correct on 
average with probability at least $0.9-\epsilon$.
\end{theorem}
In other words, compared to the situation where we know exactly who is reliable, 
we obtain the same amount of data, and its average quality is worse by at most 
$\epsilon$. Amazingly, the amount of work that each worker (and we ourselves) has 
to do does not grow with $N$; it depends only on the fraction $\alpha$ of 
reliable workers and the the desired accuracy $\epsilon$.

Our results in fact hold in a more general setting where we have $N$ raters and 
$M$ items, and a fraction $\alpha N$ of the raters are reliable; our goal is to 
recover the \emph{$\beta$-quantile} of the items --- that is, the $\beta M$ items 
that have the highest rating compared to our own judgment. In the setting above, 
the raters are the workers and the items are the data they produce. Our goal 
corresponded to recovering the $\alpha$-quantile, i.e. the data produced by the 
$\alpha N$ reliable workers.

Another scenario that our setting applies to is peer assessment in MOOCs (massive 
online open courses). Here a single class could have thousands of students, 
and to achieve scalability instructors often ask students to grade each others' 
papers \citep{kulkarni2015peer,piech2013tuned}. 
Some student graders will simply not have mastered the material themselves, 
and some enterprising but dishonest students are likely to collude, and 
so building grading systems that are robust to large errors as well as 
collusion is crucial. 
Our results apply to this setting as well, and imply that we can estimate the 
$\beta$-quantile after asking each student to grade 
$\Omega(1/\beta\alpha^3\epsilon^4)$ other student papers at random, and 
grading $\Omega(\log(1/\alpha)/\beta\epsilon^2)$ papers ourselves.
Estimating, say, the $0.1$-quantile is 
sensible if we wish to assign A's to the top $10\%$ of the class. 

Consider the following hypothetical scenario: it is the night before the 
NIPS deadline, and you are a professor with an army of $n$ graduate students, 
each of whom has written a paper.  Some fraction, $\goodraters$, of these 
students are ``highly competent''---namely, they both produce good papers and 
accurately review papers (i.e. determine whether a given paper is good 
or not).  The remaining $1-\goodraters$ fraction of your graduate students 
behave arbitrarily---some of them might still produce great papers, and some 
might still give accurate reviews.  Of course, as the professor, you are also 
capable of accurately reviewing papers.  You want to choose a large subset of 
the students' papers to submit to NIPS, at least $\goodraters n$, and you want 
to ensure that most of the submitted papers are good.  The question is how to 
choose this subset while minimizing the amount of work you do (i.e. the number 
of papers you review), as well as minimizing the number of papers you assign 
each student to review. Unfortunately, you yourself have forgotten which students 
in your lab are competent, so part of the task is to determine this.
%
Perhaps surprisingly, we show that even with an arbitrarily large number of 
students, $n$, and even if the non-competent students are all colluding to 
get their papers submitted, you can successfully accomplish this task while 
performing only a constant amount of work yourself (dependent on the fraction 
of ``highly competent'' students), and having each student only do a constant 
amount of work!

We can model this problem more precisely (as well as more generally) as follows: 
we assume that there are $m$ items and $n$ raters. We define a matrix 
$\Anom \in \{0,1\}^{n \times m}$ where $\Anom_{ij}$ denotes the rating that 
rater $i$ assigns to item $j$ ($1$ if good and $0$ if bad); in the setting above, 
the raters are graduate students and the items are papers. 
The value $\Anom_{ij}$ is observed only if rater $i$ rates item $j$. 
Similarly, we let the vector $\rtrue \in \{0,1\}^n$ denote 
our own ratings of items, which we assume are accurate; as before, 
$\rtrue_{j}$ is observed only if we rate item $j$.

We make the following assumptions:
\begin{enumerate}
\item There is a set $\good$ of honest raters of size at least $\goodfrac n$, 
      such that $\Anom_i$ agrees stochastically with $\rtrue$ in the sense that 
      $\bP[\Anom_{ij} = \rtrue_j] \geq \frac{2}{3}$ for all $i \in \sC$ and 
      $j \in [m]$, and furthermore the $\Anom_{ij}$ are independent.
\item For the dishonest raters, $\Anom_{ij}$ can be chosen adversarially, 
      with full knowledge of which ratings everyone was assigned and how the 
      honest raters behaved, and in collusion with all of the other 
      dishonest raters.
\end{enumerate}
Assuming that there are at least $\beta m$ good items, our goal is to obtain 
a set $T \subseteq [m]$ of size $\beta m$, such that 
$\frac{1}{\beta m} \sum_{j \in T} \rtrue_j \geq 1-\epsilon$.
The reults of this paper imply that we can do this with minimal work per rater, 
as long as the number of raters is comparable to the number of items:
\begin{theorem}
\label{thm:main-1}
Suppose that each rater is assigned $k$ items to rate at random, and that we rate 
$k_0$ items at random. Then, assuming that $k \geq \Omega\p{\frac{1}{\alpha^3\beta\epsilon^4} + \frac{\log(1/\delta)}{\alpha\beta\epsilon^2}}$, and 
$k_0 \geq \Omega\p{\frac{\log(1/\alpha\delta)}{\alpha\beta\epsilon^2}}$, 
with probability $1-\delta$ we can recover a set $T$ of size $\gooditems m$, 
such that $\frac{1}{\gooditems m} \sum_{j \in T} \rtrue_j \geq 1-\epsilon$.
\end{theorem}
The main computational step in the recovery algorithm involves solving a 
nuclear-norm constrained optimization problem (where all of the remaining 
constraints are linear inequalities). \todo{talk about runtime?}

% Overview of Related Work
% Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems
%  {karger2014budget}
%  uses belief propagation to model worker reliability, assign workers to tasks
% Approval Voting and Incentives in Crowdsourcing
%  {shah2015approval}
% Double or nothing: Multiplicative incentive mechanisms for crowdsourcing
%  {shah2015double}
%  same as above
% Regularized Minimax Conditional Entropy for Crowdsourcing
%  {zhou2015regularized}
%  models worker reliability and task difficulty
% Truth Serums for Massively Crowdsourced Evaluation Tasks
%  {kamble2015truth}
% Eliciting Informative Feedback: The Peer-Prediction Method
%  -- in some sense, defined our problem setting
%  -- very central paper
%  {miller2005eliciting}
%  other related papers: {shnayder2016strong}, {dasgupta2013crowdsourced}

% Evidence for our mechanism being important
% {harmon2004amazon} --- manipulation of book reviews
% {mayzlin2006promotional,white1999chatting} --- manipulation of online bulletin boards
% MOOCs 
%   Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and
%   Daphne Koller. Tuned models of peer assessment in MOOCs. arXiv preprint
%   arXiv:1307.2579, 2013.
% Crowdsourcing
%   Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, 
%   Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. 
%   The Journal of Machine Learning Research, 11:1297â€“1322, 2010.
% Yelp
%   Michael Luca. Reviews, reputation, and revenue: The case of yelp. com. Com
%   (September 16, 2011). Harvard Business School NOM Unit Working Paper, 
%   (12-016), 2011.
% {dellarocas2006strategic} --- general survey
% The Influence Limiter: Provably Manipulation-Resistant Recommender Systems
%  seems very similar to ours, but they assume online feedback of ground truth
%  {resnick2007influence}

Beyond the professorial scenario above, the above setup
models the challenge of crowdsourcing the creation of a large 
dataset, as well as the more general problem of \emph{peer prediction} 
\citep{miller2005eliciting}, in which we wish to obtain truthful information 
from a population of raters by exploiting inter-rater agreement. 
While several mechanisms have been proposed for these tasks, 
they typically assume that there are ``gold labels'' that can be used 
to assess rater performance \citep{resnick2007influence}, that raters are 
rational agents maximizing a payoff function \citep{dasgupta2013crowdsourced,
kamble2015truth,shnayder2016strong}, that the workers follow a simple 
statistical model \citep{karger2014budget,zhang2014crowdsourcing,
zhou2015regularized}, or some combination of the above \citep{shah2015double,
shah2015approval}. 

In this work we are motivated by cases where there is not necessarily a single 
correct answer --- for instance, tasks such as ``draw an interesting picture'' 
or ``translate this sentence'', where different workers could generate different 
equally good answers. In this case, the only way to evaluate quality is 
via evaluation by other workers or by the manager of the experiment. 
Moreover, since workers will often try to game the system (in order to 
get paid), we would like to be robust to adversarial behavior among 
non-cooperative workers, including collusion.

Beyond crowdsourcing, the problem of ensuring rating quality in the presence 
of adversaries has appeared in a variety of applications:
\begin{enumerate}
\item \textbf{Massive open online courses (MOOCs):} in massive open online 
      courses, instructors often rely on peer-grading to achieve scalability 
      \citep{piech2013tuned}. In this setting, there are clear incentives for 
      students to collude in order to obtain inflated grades, so robustness to 
      adversarial behavior is important.
\item \textbf{Amazon book reviews:} \citet{harmon2004amazon} reports that 
      many authors and filmmakers engage in dishonest tactics to inflate 
      their reviews on amazon.com.
\item \textbf{Online bulletin boards:} \citet{white1999chatting} reports that 
      record companies pose as regular users on music forums to promote their 
      artists.
%\item \textbf{Twitter \& Facebook:} --- this might not be that compelling; 
%   look at "Trafficking Fraudulent Accounts: 
%            The Role of the Underground Market in Twitter Spam and Abuse" 
% yelp?
\item \textbf{Wikipedia:} \citet{priedhorsky2007creating} find that roughly 
      $5\%$ of revisions on 
      wikipedia are damaged in some way (e.g. by vandalism). Because wikipedia 
      is primarily moderated by the crowd, building resilient ratings mechanisms 
      is important for maintaining article quality.
\end{enumerate}


While we have stated a very concrete setup for simplicity, our results 
hold in a substantially more general setting that we call \textbf{quantile 
estimation}. The ratings are now allowed to lie in $[0,1]$, and 
our goal is to recover the \emph{$\beta$-quantile} $Q_{\beta}$ of 
$\rtrue$, defined as the $\beta m$ indices $j$ for which $\rtrue_j$ is largest. 
In order to do this, we need to make $3$ assumptions, which are relaxations of 
the assumptions above: (1) the $\beta$-quantile of each of the honest raters 
matches the $\beta$-quantile of $\rtrue$; (2) the ratings in $\rtrue$ are a 
Lipschitz function of the average rating 
$\frac{1}{|\good|}\sum_{i \in \good} \Anom_i$; and (3) the ratings assigned by 
the honest raters are sufficiently independent for typical matrix 
concentration bounds to hold. We formalize these assumptions in 
Section~\ref{sec:assumptions}. \todo{talk about stochastic block model}

The rest of the paper is organized as follows. In Section~\ref{sec:assumptions}, 
we state our formal assumptions and main results, and show some concrete 
settings in which they apply. In Section~\ref{sec:approach}, we outline a proof 
of our main results and present a concrete algorithm for $\beta$-quantile 
estimation, deferring the more technical proofs until later. 
In Section~\ref{sec:subgradient-proof}, we provide the proof of 
Proposition~\ref{prop:subgradient}, which is the key result bounding the effect 
that the adversaries can have on the solution quality. We end in 
Section~\ref{sec:discussion} with a discussion.
