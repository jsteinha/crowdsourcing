\section{Introduction}
\label{sec:intro}

% References:
% An Analysis of Assessor Behavior in Crowdsourced Preference Judgments
% Dongqing Zhu and Ben Carterette {zhu2010analysis}

% NEW INTRO
How can we reliably obtain information from humans, given that the humans 
themselves are unreliable, and might even have incentives to mislead us?
Versions of this question arise in crowdsourcing \citep{vuurens2011spam}, 
collaborative knowledge generation \citep{priedhorsky2007creating}, peer grading 
in online classrooms \citep{kulkarni2015peer,piech2013tuned}, and aggregation 
of customer reviews \citep{harmon2004amazon}. A key challenge is to ensure 
high information quality despite the fact that many people interacting with 
the system may be unreliable or even adversarial.

% talk about gold sets --- say not okay because too constrainint
One approach to ensuring quality is to use \emph{gold sets} --- questions where 
the answer is known, which can be used to assess reliability on unknown questions. 
However, this is overly constraining --- it does not make sense for open-ended 
tasks such as knowledge generation on wikipedia, nor even for crowdsourcing 
tasks such as ``translate this sentence'' or ``draw an interesting picture'' 
where there are different equally good answers. 
In this work, we take a different perspective: we make no restrictions on the 
content that people can produce, and instead focus on \emph{evaluating} this 
content, using human ratings to achieve scalability in this evaluation task. 
For instance, in a crowdsourcing setting where we wanted to translate sentences 
from Chinese to English, we would first ask workers to perform the translation, 
and then ask a (potentially) new set of workers to evaluate the quality of 
the translations.

% our insight --- let humans rate other humans!
% challenges: different scales, different responses to different items
This leads to several challenges. First, many workers may be unreliable; 
second, some reliable raters might be harsher or more lenient than others; 
third, some outputs may be harder to evaluate than others 
%(e.g. long vs. short sentences) 
and so rater error rates could vary from item to item. 
Finally, some 
workers may even collude (e.g. by agreeing to rate each other's output highly).
On the other hand, depiste all these difficulties, at least some of the workers 
actually are reliable. This raises the question: can we somehow extract the 
information from the reliable workers, without knowing who they are a priori?

In this work, we answer this question in the affirmative, under surprisingly 
weak assumptions:
\begin{itemize}
\item We do not assume that there is a ``gold set'' or other cheap way to judge 
      worker performance;instead, we rely on a small number of our own post hoc judgments.
\item We do not assume that the majority of workers are reliable.
\item We do not assume that the unreliable workers conform to any statistical 
      model; they could behave fully adversarially, in collusion with each other 
      and with full knowledge of how the reliable workers behave.
\item We do not assume that worker ratings match our own, but only that they are 
      ``approximately monotonic'' in our ratings, in a sense that will be 
      formalized later.
\end{itemize}
For concreteness, we describe a simple formalization of the crowdsourcing 
setting (our actual results hold in a more general setting). We assume 
there are $n$ workers (raters) and $n$ outputs to evaluate (items), which can
be correct or incorrect. At least $\alpha n$ workers are reliable:
they judge correctness with at least $60\%$ accuracy on each item 
and make independent errors. Each worker is assigned to judge a randomly selected 
output from between $k$ and $2k$ other workers (also chosen at random). In 
addition, we ourselves judge an output from each of $k_0$ workers. Our results 
imply the following:

\input matrix-figure

\begin{theorem}
\label{thm:intro}
In the setting above, suppose that $\beta n$ of the outputs are correct, that 
$k \geq \Omega(1/\beta\alpha^3\epsilon^4)$, and that 
$k_0 \geq \Omega(\log(1/\alpha\beta\epsilon)/\beta\epsilon^2)$. Then, with probability 
at least $99\%$, we can recover $\beta n$ outputs of which at most $\epsilon\beta n$ 
are incorrect.
\end{theorem}
Amazingly, the amount of work that each worker (and we ourselves) has 
to do does not grow with $n$; it depends only on the fraction $\alpha$ of 
reliable workers and the the desired accuracy $\epsilon$.
Our general results allow for scalar ratings lying in $[0,1]$, and only 
exploit inter-rater agreement (the expected value of the ratings induce 
the same ranking) and independent errors among the reliable raters.

Our results are also applicable to peer grading in massive online open courses. 
A single class could have thousands of students, 
and to achieve scalability instructors often ask students to grade each others' 
papers \citep{kulkarni2015peer,piech2013tuned}. 
Some student graders will simply not have mastered the material themselves, 
and some enterprising but dishonest students are likely to collude, 
so building grading systems that are robust to large errors as well as 
collusion is crucial. 
An important aspect of our results is that they hold even if some graders are 
more lenient than others, as long as the \emph{ranking} produced by the graders 
is the same. The focus on quantiles rather than raw scores is what enables this. 
Note that once we estimate the quantiles, we can approximately recover the 
raw scores by evaluating a few items in each quantile (though for some purposes, 
such as assigning an A to the top $10\%$ of the class, the quantiles already suffice).

% Overview of Related Work
% Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems
%  {karger2014budget}
%  uses belief propagation to model worker reliability, assign workers to tasks
% Approval Voting and Incentives in Crowdsourcing
%  {shah2015approval}
% Double or nothing: Multiplicative incentive mechanisms for crowdsourcing
%  {shah2015double}
%  same as above
% Regularized Minimax Conditional Entropy for Crowdsourcing
%  {zhou2015regularized}
%  models worker reliability and task difficulty
% Truth Serums for Massively Crowdsourced Evaluation Tasks
%  {kamble2015truth}
% Eliciting Informative Feedback: The Peer-Prediction Method
%  -- in some sense, defined our problem setting
%  -- very central paper
%  {miller2005eliciting}
%  other related papers: {shnayder2016strong}, {dasgupta2013crowdsourced}

% Evidence for our mechanism being important
% {harmon2004amazon} --- manipulation of book reviews
% {mayzlin2006promotional,white1999chatting} --- manipulation of online bulletin boards
% MOOCs 
%   Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, and
%   Daphne Koller. Tuned models of peer assessment in MOOCs. arXiv preprint
%   arXiv:1307.2579, 2013.
% Crowdsourcing
%   Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, 
%   Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. 
%   The Journal of Machine Learning Research, 11:1297â€“1322, 2010.
% Yelp
%   Michael Luca. Reviews, reputation, and revenue: The case of yelp. com. Com
%   (September 16, 2011). Harvard Business School NOM Unit Working Paper, 
%   (12-016), 2011.
% {dellarocas2006strategic} --- general survey
% The Influence Limiter: Provably Manipulation-Resistant Recommender Systems
%  seems very similar to ours, but they assume online feedback of ground truth
%  {resnick2007influence}

% TODO: who to cite?
% Improved Graph Clustering {chen2014improved}
% A Proof Of The Block Model Threshold Conjecture
% Community detection thresholds and the weak ramanujan property.
%  Community detection in sparse networks via Grothendieck_0s inequality
Our technical tools draw on semidefinite programming methods for matrix 
completion, which have been used to study graph clustering as well 
as the stochastic block model \citep{holland1983stochastic,condon2001algorithms}. 
Our setting corresponds to the sparse case where all nodes have constant degree, 
which has recently seen great interest \citep{decelle2011asymptotic,
mossel2012stochastic,mossel2013proof,mossel2013belief,
massoulie2014community,guedon2014community,mossel2015consistency,
chin2015stochastic,abbe2015community,makarychev2015learning}. 
\citet{makarychev2015learning} in particular provide an algorithm that is 
robust to adversarial perturbations, but only if the perturbation has 
size $o(n)$; see also \citet{cai2015robust} for robusness results when 
the node degree is logarithmic.
% NOTE: decelle2011asymptotic make a conjecture that implies 
% 1/alpha^2epsilon^2 is tight even in a very simple case

Several authors have considered semirandom settings for graph clustering, which 
allow for some types of adversarial behavior \citep{feige2000finding,
feige2001heuristics,coja2004coloring,krivelevich2006semirandom,
coja2007solving,makarychev2012approximation,chen2014improved,guedon2014community,
moitra2015robust,agarwal2015multisection}. 
In our setting, these semirandom models would need to assume that the adversaries 
are strictly dominated by the reliable raters (in the sense of having lower 
expected accuracy on every item); this is implausible as it rules out 
most types of strategic behavior.
In removing this assumption, we face a key technical challenge: while previous 
analyses consider errors relative to a ground truth clustering 
\citep[e.g.][]{chen2014improved}, in our setting 
the ground truth only exists for rows of the matrix corresponding to reliable 
raters, while the remaining rows could behave arbitrarily even in the limit 
where all ratings are observed. This necessitates a more careful analysis, 
which has the advantage of clarifying which properties of previous clustering 
models were necessary and which were superfluous.


\paragraph{Overview of the paper.} In Section~\ref{sec:algorithm}, we 
present our algorithm and explain the intuition behind it, as well as why 
some similar algorithms would be less robust to adversaries. In 
Section~\ref{sec:approach}, we present our formal assumptions and explain 
the key ingredients in our proof, with technical details deferred to the 
appendix. In Section~\ref{sec:discussion}, we discuss our results and 
place them in the context of existing work.
