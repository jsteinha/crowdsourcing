\section{Recovering $T$ (Algorithm~\ref{alg:recover-T})}
\vskip -0.10in
\label{sec:approach-T}
\label{sec:rounding}

In this section we show that if $\M$ satisfies the conclusion of 
Proposition~\ref{prop:recover-M}, then Algorithm~\ref{alg:recover-T} 
recovers a set $T$ that approximates $T^*$ well. We treat $T$ and $T^*$ 
as $\{0,1\}$-vectors, and use the notation $\langle T, r \rangle$ 
to denote $\sum_{j \in [m]} T_jr_j$. Formally, we show the following:
\begin{proposition}
\label{prop:recover-T}
%Suppose that the output $\M$ of Algorithm~\ref{alg:recover-M} satisfies 
%$\frac{1}{|\good|} \frac{1}{\beta m}\sum_{i \in \good}\sum_{j \in [m]} \M_{ij}\ravg_j \geq \p{\frac{1}{\beta m} \sum_{j \in T^*} \ravg_j} - \epsilon_1$. 
Suppose Assumption~\ref{ass:independent} holds. For some 
$k_0 = \oo\p{\frac{\log(2/\alpha\beta\epsilon\delta)}{\beta\epsilon^2}}$, 
with probability $1-\delta$, Algorithm~\ref{alg:recover-T} outputs a set $T$ satisfying 
\begin{equation}
\label{eq:recover-T}
\langle T^* - T, \ravg \rangle  \leq \frac{2}{|\good|}\Big(\sum_{i \in \good} \langle T^* - \M_{i}, \ravg \rangle\Big) + \epsilon\beta m.
\end{equation}
\end{proposition}
To establish Proposition~\ref{prop:recover-T}, 
first note that with probability $1-\delta$, 
at least one of the $\frac{2\log(1/\delta)}{\alpha}$ randomly selected 
$i$ will have cost $\langle T^* - \M_i, \ravg \rangle$ 
within twice the average cost across $\good$. This 
is because with probability $\alpha$, a randomly selected 
$i$ will lie in $\good$, and with probability $\frac{1}{2}$, 
an $i \in \good$ will have cost at 
most twice the average cost (by Markov's inequality).

The remainder of the proof hinges on two results. First, we establish 
a concentration bound showing that $\sum_j \M_{ij}\robs_j$ is close to 
$\frac{k_0}{m}\sum_j \M_{ij}\rtrue_j$ for any fixed $i$, and hence (by a union bound) 
also for the $\frac{2\log(1/\delta)}{\alpha}$ randomly selected $i$. 
This yields the following lemma, which is a straightforward 
application of a Chernoff bound (see Section~\ref{sec:concentration-proof} for a proof):
\begin{lemma}
\label{lem:robs-rtrue}
Let $i^*$ be the row selected in Algorithm~\ref{alg:recover-T}. 
Suppose that $\robs$ satisfies Assumption~\ref{ass:independent}. 
For some 
$k_0 = \oo\p{\frac{\log(2/\alpha\failprob)}{\beta\epsilon^2}}$, 
with probability $1-\delta$, we have
\vskip -0.15in
\begin{equation}
\label{eq:robs-rtrue}
\langle T^*-\M_{i^*}, \rtrue_j \rangle \leq \frac{2}{|\good|} \sum_{i \in \good} \langle T^* - \M_i, \rtrue \rangle + \frac{\epsilon}{4}\beta m.
\end{equation}
\end{lemma}
\vskip -0.1in
%The idea is to establish a uniform bound showing that 
%$\sum_{i \in S} \sum_{j \in [m]} \M_{ij}(\robs_j - \frac{k_0}{m}\rtrue_j)$ is small for any 
%set of $\goodfrac n$ rows $S$, and hence that greedily taking the $\goodfrac n$ 
%best rows according to $\robs$ is almost as good as taking the $\goodfrac n$ 
%best rows according to $\rtrue$. We 
%%Our analysis follows that of \citetm{todo}, 
%improve over a na\"{i}ve union bound by exploiting power mean 
%inequalities on cumulant functions. 
%\todo{Greg can you fill in this citation?}

Having recovered a good row $T_0 = \M_{i^*}$, 
we need to turn $T_0$ into a binary vector so that 
Algorithm~\ref{alg:recover-T} can output a set;
we do so via randomized rounding, obtaining a vector $T \in \{0,1\}^m$ such that 
$\bE[T] = T_0$ (where the randomness is with respect to choices made by the algorithm).
Our rounding procedure is given in Algorithm~\ref{alg:round}; the following 
lemma, proved in \ref{sec:rounding-proof}, asserts its correctness:
\begin{lemma}
\label{lem:rounding}
The output $T$ of Algorithm~\ref{alg:round} satisfies $\bE[T] = T_0$, 
$\|T\|_0 \leq \beta m$.
\end{lemma}

\input round

The remainder of the proof involves lower-bounding the probability 
that $T$ is accepted in each stage of the while loop in 
Algorithm~\ref{alg:recover-T}.
We refer the reader to Section~\ref{sec:recover-T-proof} for details.
%Recall that in Algorithm~\ref{alg:recover-T}, we accept $T$ if 
%$\langle T, \robs' \rangle \geq \langle T_0, \robs' \rangle - \frac{1}{4}\epsilon\beta k_0$. Since $\langle T, \robs' \rangle \in [0,k_0]$ almost surely, and 
%$\bE[\langle T, \robs' \rangle] = \langle T_0, \robs' \rangle$, 
%Markov's inequality 
%implies that we accept with probability at least $\frac{\beta \epsilon}{4}$ on 
%each round, and so with probability $1-\delta$ have at most 
%$\frac{4\log(1/\delta)}{\epsilon\beta}$ rounds. By a union bound, 
%$|\langle T, \robs' - \frac{k_0}{m}\ravg \rangle| \leq \frac{\epsilon}{4}$ 
%for all of the rounds, implying that the accepted set $T$ satisfies 
%\begin{alignat}{2}
%\notag \sum_{j \in T} \ravg_j &\geq \frac{m}{k_0}\sum_{j \in T} \robs'_j - \frac{\epsilon}{4}\beta m &&\geq \frac{m}{k_0}\sum_{j \in [m]}(T_0)_j\robs'_j  - \frac{2\epsilon}{4}\beta m \\
% &\geq \sum_{j \in [m]} (T_0)_j \ravg_j - \frac{3\epsilon}{4}\beta m &&\geq \sum_{j \in T^*} \ravg_j - (\epsilon_1+\epsilon),
%\end{alignat}
%which is what we wanted to show in Proposition~\ref{prop:recover-T}. We refer 
%the reader to Section~\ref{sec:recover-T-proof} for a more detailed proof.
%
