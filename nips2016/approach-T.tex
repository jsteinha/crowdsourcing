\section{Recovering $T$ (Algorithm~\ref{alg:recover-T})}
\label{sec:approach-T}
\label{sec:rounding}

In this section we show that if $\M$ satisfies the conclusion of 
Proposition~\ref{prop:recover-M}, then Algorithm~\ref{alg:recover-T} 
recovers a set $T$ that approximates $T^*$ well. Formally, we show 
the following:
\begin{proposition}
\label{prop:recover-T}
%Suppose that the output $\M$ of Algorithm~\ref{alg:recover-M} satisfies 
%$\frac{1}{|\good|} \frac{1}{\beta m}\sum_{i \in \good}\sum_{j \in [m]} \M_{ij}\ravg_j \geq \p{\frac{1}{\beta m} \sum_{j \in T^*} \ravg_j} - \epsilon_1$. 
Suppose that Assumption~\ref{ass:independent} holds and that 
$k_0 \geq \Omega\p{\frac{\log(1/\alpha\beta\epsilon\delta)}{\beta\epsilon^2}}$. 
Then, Algorithm~\ref{alg:recover-T} outputs a set $T$ satisfying 
\begin{equation}
\label{eq:recover-T}
\frac{1}{\beta m}\sum_{j \in T} \ravg_j \geq \p{\frac{1}{\beta m}\frac{1}{|\good|}\sum_{i \in \good} \sum_{j \in [m]} \M_{ij}\ravg_j} - \epsilon.
\end{equation}
\end{proposition}
The validity of this procedure hinges on two results. First, we need to establish 
a concentration bound showing that $\sum_j \M_{ij}\robs_j$ is close to 
$\frac{k_0}{m}\sum_j \M_{ij}\rtrue_j$ for all $i \in \good$, which implies that 
the $\goodfrac n$ best rows of $\M$ according to $\robs$ also look good 
according to $\robs$. This is captured in the following lemma:
\begin{lemma}
\label{lem:robs-rtrue}
Suppose that $\robs$ satisfies Assumption~\ref{ass:independent} and that 
$k_0 \geq \Omega\p{\frac{\log(1/\alpha\failprob)}{\beta\epsilon^2}}$. 
Then, with probability $1-\delta$, we have
\begin{equation}
\label{eq:robs-rtrue}
\frac{1}{\goodfrac n} \sum_{i \in \goodapprox} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) \geq \frac{1}{|\good|} \sum_{i \in \good} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) - \frac{\epsilon}{4}\beta m.
\end{equation}
\end{lemma}
See Section~\ref{sec:concentration-proof} for a proof.
The idea is to establish a uniform bound showing that 
$\sum_{i \in S} \sum_{j \in [m]} \M_{ij}(\robs_j - \rtrue_j)$ is small for any 
set of $\goodfrac n$ rows $S$, and hence that greedily taking the $\goodfrac n$ 
best rows according to $\robs$ is almost as good as taking the $\goodfrac n$ 
best rows according to $\rtrue$. Our analysis follows that of 
\citet{todo}, improving over a na\"{i}ve union bound by exploiting power mean 
inequalities for the cumulant function. \todo{Greg can you fill in this 
citation?}

Once we have recovered a set $\goodapprox$ of good rows, we need to turn this 
into an actual set $T \subset [m]$. To this end, first let $T_0 \in [0,1]^m$ 
be defined as $T_0 \eqdef \frac{1}{|\goodapprox|} \sum_{i \in \goodapprox} \M_i$. 
We will use randomized rounding to obtain a vector $T \in \{0,1\}^m$ such that 
$\bE[T_0] = T$; $T$ is then the indicator function of our desired set.
Our rounding procedure is given in Algorithm~\ref{alg:round}; the following 
lemma, proved in \ref{sec:rounding-proof}, asserts its correctness:
\begin{lemma}
\label{lem:rounding}
The output $T$ of Algorithm~\ref{alg:round} satisfies $\bE[T] = T_0$, 
$\|T\|_0 \leq \beta m$.
\end{lemma}

\input round

Recall that in Algorithm~\ref{alg:recover-T}, we accept $T$ if 
$\langle T, \robs' \rangle \geq \langle T_0, \robs' \rangle - \frac{1}{4}\epsilon\beta k_0$. Since $\langle T, \robs' \rangle \in [0,k_0]$ almost surely, and 
$\bE[\langle T, \robs' \rangle] = \langle T_0, \robs' \rangle$, 
Markov's inequality 
implies that we accept with probability at least $\frac{\beta \epsilon}{4}$ on 
each round, and so with probability $1-\delta$ have at most 
$\frac{4\log(1/\delta)}{\epsilon\beta}$ rounds. By a union bound, 
$|\langle T, \robs' - \frac{k_0}{m}\ravg \rangle| \leq \frac{\epsilon}{4}$ 
for all of the rounds, implying that the accepted set $T$ satisfies 
\begin{alignat}{2}
\notag \sum_{j \in T} \ravg_j &\geq \frac{m}{k_0}\sum_{j \in T} \robs'_j - \frac{\epsilon}{4}\beta m &&\geq \frac{m}{k_0}\sum_{j \in [m]}(T_0)_j\robs'_j  - \frac{2\epsilon}{4}\beta m \\
 &\geq \sum_{j \in [m]} (T_0)_j \ravg_j - \frac{3\epsilon}{4}\beta m &&\geq \sum_{j \in T^*} \ravg_j - (\epsilon_1+\epsilon),
\end{alignat}
which is what we wanted to show in Proposition~\ref{prop:recover-T}. We refer 
the reader to Section~\ref{sec:recover-T-proof} for a more detailed proof.

