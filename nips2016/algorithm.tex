\section{Algorithm}
\label{sec:algorithm}


We now describe our recovery algorithm. To fix notation, we assume that 
there are $n$ raters and $m$ items, and that we observe a matrix 
$\A \in [0,1]^{n \times m}$, obtained by asking each rater to rate $k$ 
items chosen at random: $\A_{ij} = 0$ if rater $i$ does not rate item 
$j$, and otherwise $\A_{ij}$ is the assigned rating, which takes values 
in $[0,1]$. In the settings we 
care about $\A$ is very sparse --- it will only have a constant number of 
non-zero entries in each row. Remember that our goal is to recover the 
$\beta m$ best items according to our own rating (while only actually rating a 
constant number of items).

Our algorithm is based on the following intuition: the reliable raters must 
(approximately) agree with each other, and so if we can somehow cluster the 
rows of $\A$, then the reliable raters should form a single very large cluster 
(of size $\alpha n$). There can only be $\frac{1}{\alpha}$ such clusters, and 
so we can manually check the accuracy of each large cluster (for instance, 
by randomly selecting some ratings and comparing them to our own rating) and 
then choose the best one.
Our algorithm differs from this outline due to various 
optimizations, but adheres in spirit to the fundamental idea. 

\input recover-M

One major challenge in using the clustering intuition is the sparsity of 
$\A$: any two rows of $\A$ will almost certainly have no ratings in common, 
and so we must somehow exploit the global structure of $\A$ to discover 
clusters, rather than using pairwise comparisons of rows.
The key is to view our problem as a form of \emph{noisy matrix completion} --- 
we imagine a matrix $\Aavg$ in which all of the ratings have been filled in 
and all noise from individual ratings has been removed. We can then define a 
matrix $\Mavg$ that indicates the top $\beta m$ items in each row of $\Aavg$: 
$\Mavg_{ij} = 1$ if item $j$ has one of the top $\beta m$ ratings from rater $i$, 
and $\Mavg_{ij} = 0$ otherwise. If we could recover $\Mavg$, we would be very 
close to obtaining the clustering we wanted.

The key observation that allows us to recover $\Mavg$ given only the noisy, 
incomplete $\A$ is that \emph{$\Mavg$ has low-rank structure}: since all 
of the reliable raters agree with each other, their rows in $\Mavg$ are all 
identical, and so there is an $(\alpha n) \times m$ submatrix of $\Mavg$ with 
rank $1$. This inspires the low-rank matrix completion algorithm for recovering 
$\M$ given in Algorithm~\ref{alg:recover-M}. Each row of $M$ is constrained 
to have sum at most $\beta m$, and $M$ as a whole is constrained to have 
nuclear norm $\|M\|_*$ at most $\frac{2}{\alpha \epsilon}\sqrt{\alpha\beta nm}$. 
Recall that the \emph{nuclear norm} is the sum of the singular values of 
$M$; in the same way that the $\ell^1$-norm is a convex surrogate for the 
$\ell^0$-norm, the nuclear norm acts as a convex surrogate for the rank of $M$ 
(i.e., number of non-zero singular values). The optimization problem 
\eqref{eq:optimization-noisy} therefore chooses a set of $\beta m$ items in each 
row to maximize the corresponding values in $\A$, while constraining the item 
sets to have low rank (where low rank is relaxed to low nuclear norm to obtain 
a convex problem). 
This low-rank constraint acts as a strong regularizer that quenches the noise 
in $\A$.

\input recover-T

Once we have recovered $\M$ using Algorithm~\ref{alg:recover-M}, it remains to 
recover a specific set $T$ that approximates the $\beta$-quantile according to 
our ratings. Algorithm~\ref{alg:recover-T} provides a simple recipe for doing so: 
first, rate $k'$ items at random, obtaining a vector $\robs$ similar to to 
$\Aobs$: $\robs_j = 0$ if we did not rate item $j$, and otherwise $\robs_j$ is 
the (possibly noisy) rating that we assign to item $j$. Second, pick the 
$\alpha n$ rows whose item sets score highest according to our rating: that is, 
the indices $i$ for which $\sum_j \M_{ij}\robs_j$ is highest. Finally, let 
$T_0$ be the average of these $\M_i$. The only catch is that $T_0$ lies in 
$[0,1]^m$ rather than $\{0,1\}^m$, so it is not necessarily the indicator 
function of a set. To remedy this, we use a randomized rounding algorithm to 
obtain a set $T$ whose quality matches $T_0$ in expectation. This algorithm 
is described in detail in Section~\ref{sec:rounding}.

In summary, given a noisy rating matrix $\Aobs$, we will first run 
Algorithm~\ref{alg:recover-M} to recover a $\beta$-quantile matrix $\M$ for 
each rater, and then run Algorithm~\ref{alg:recover-T} to recover our 
personal $\beta$-quantile from $\M$.

\paragraph{Possible attacks by adversaries.} We note the necessity of a 
few aspects of our algorithm by exhibiting attacks that would be possible 
in our absence. First, the only attack vector by which adversaries can 
influence $M_i$ for reliable raters $i$ is via the nuclear norm constraint 
(the other constraints are separable across rows). This makes sense because 
the nuclear norm is what causes us to pool global structure across 
raters (and thus potentially pool bad information). The constraint on the 
nuclear norm is weaker than is typical by a factor of $\frac{2}{\epsilon}$; 
this looseness limits the influence of adversaries at the expense of more 
work by the honest raters.

Second, why is it necessary to include some of our own ratings? 
If we did not, and $\alpha < \frac{1}{2}$, then an adversary could create a set of 
dishonest raters that were identical to the reliable raters except with the 
item indices permuted by a random permutation of $\{1,\ldots,m\}$. In this case, 
there is no way to distinguish the honest from the dishonest raters except by 
breaking the symmetry with our own ratings.

Finally, the constraint $\sum_j M_{ij} \leq \beta m$ is 
not typical in the literature. For instance, \citep{chen2014improved} 
instead normalize $\A$ to lie in $[-1,1]^{m \times m}$ but place no 
constraint on the sum of each row in $M$ (instead of recovering 
the $\beta$-quantile, they recover the set of items with positive rating).
Our row normalization constraint is necessary to prevent a type of attack 
in which a spammer rates a random subset of items as high as possible and 
rates the remaining items as low as possible. If the actual set of 
high-quality items has density much smaller than $50\%$, then the 
spammer might have undue influence relative to honest raters that 
only give high ratings to e.g. $10\%$ of the items. Normalizing $M$ to 
have the same total sum in each row prevents this (see 
Appendix~\ref{sec:normalization} for a more formal version of this argument).
