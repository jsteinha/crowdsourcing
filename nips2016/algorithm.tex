\section{Algorithm and Intuition}
\label{sec:algorithm}

\input recover-M

We now describe our recovery algorithm. To fix notation, we assume that 
there are $n$ raters and $m$ items, and that we observe a matrix 
$\A \in [0,1]^{n \times m}$: 
%obtained by asking each rater to rate at most $k$ 
%items chosen at random: 
$\A_{ij} = 0$ if rater $i$ does not rate item $j$, and otherwise $\A_{ij}$ 
is the assigned rating, which takes values 
in $[0,1]$. In the settings we 
care about $\A$ is very sparse --- each rater only rates a few items.
Remember that our goal is to recover the $\beta$-quantile $T^*$ of the 
best items according to our own rating. 
% TODO move this later
%In what follows, we will represent 
%$T^*$ as a vector in $\{0,1\}^m$: $T_j^* = 1$ if $j$ is one of the top 
%$\beta m$ items, and $T_j^* = 0$ otherwise.

Our algorithm is based on the following intuition: the reliable raters must 
(approximately) agree on the ranking of items, and so if we can cluster the 
rows of $\A$ appropriately, then the reliable raters should form a single very large cluster 
(of size $\alpha n$). There can be at most $\frac{1}{\alpha}$ disjoint clusters of this size, and 
so we can manually check the accuracy of each large cluster (by checking agreement 
with our own rating on a few randomly selected items) and 
then choose the best one.

One major challenge in using the clustering intuition is the sparsity of 
$\A$: any two rows of $\A$ will almost certainly have no ratings in common, 
so we must exploit the global structure of $\A$ to discover 
clusters, rather than using pairwise comparisons of rows.
The key is to view our problem as a form of \emph{noisy matrix completion} --- 
we imagine a matrix $\Aavg$ in which all the ratings have been filled in 
and all noise from individual ratings has been removed. We define a 
matrix $\Mavg$ that indicates the top $\beta m$ items in each row of $\Aavg$: 
$\Mavg_{ij} = 1$ if item $j$ has one of the top $\beta m$ ratings from rater $i$, 
and $\Mavg_{ij} = 0$ otherwise (this differs from the actual 
definition of $\Mavg$ given in Section~\ref{sec:approach-M}, but is the same in spirit). If we could recover $\Mavg$, we would be 
close to obtaining the clustering we wanted.

The key observation that allows us to approximate $\Mavg$ given only the noisy, 
incomplete $\A$ is that \emph{$\Mavg$ has low-rank structure}: since all 
of the reliable raters agree with each other, their rows in $\Mavg$ are all 
identical, and so there is an $(\alpha n) \times m$ submatrix of $\Mavg$ with 
rank $1$. This inspires the low-rank matrix completion algorithm for recovering 
$\M$ given in Algorithm~\ref{alg:recover-M}. Each row of $M$ is constrained 
to have sum at most $\beta m$, and $M$ as a whole is constrained to have 
nuclear norm $\|M\|_*$ at most $\frac{2}{\alpha \epsilon}\sqrt{\alpha\beta nm}$. 
Recall that the \emph{nuclear norm} is the sum of the singular values of 
$M$; in the same way that the $\ell^1$-norm is a convex surrogate for the 
$\ell^0$-norm, the nuclear norm acts as a convex surrogate for the rank of $M$ 
(i.e., number of non-zero singular values). The optimization problem 
\eqref{eq:optimization-noisy} therefore chooses a set of $\beta m$ items in each 
row to maximize the corresponding values in $\A$, while constraining the item 
sets to have low rank (where low rank is relaxed to low nuclear norm to obtain 
a convex problem). 
This low-rank constraint acts as a strong regularizer that quenches the noise 
in $\A$.

\input recover-T

Once we have recovered $\M$ using Algorithm~\ref{alg:recover-M}, it remains to 
recover a specific set $T$ that approximates the $\beta$-quantile according to 
our ratings. Algorithm~\ref{alg:recover-T} provides a recipe for doing so: 
first, rate $k_0$ items at random, obtaining the vector $\robs$:
$\robs_j = 0$ if we did not rate item $j$, and otherwise $\robs_j$ is 
the (possibly noisy) rating that we assign to item $j$. Next, score each 
row $\M_{i}$ based on the noisy ratings $\sum_j \M_{ij}\robs_j$, and let 
$T_0$ be the highest-scoring $\M_i$ among $\oo(\log(2/\alpha)/\epsilon^2)$ randomly 
selected $i$.
Finally, randomly round the vector $T_0 \in [0,1]^m$ to 
a discrete vector $T \in \{0,1\}^m$, and treat $T$ as the indicator function 
of a set approximating the $\beta$-quantile
(see Section~\ref{sec:rounding} for details of the rounding algorithm).

In summary, given a noisy rating matrix $\Aobs$, we will first run 
Algorithm~\ref{alg:recover-M} to recover a $\beta$-quantile matrix $\M$ for 
each rater, and then run Algorithm~\ref{alg:recover-T} to recover our 
personal $\beta$-quantile from $\M$.

\paragraph{Possible attacks by adversaries.} In our algorithm, 
the adversaries can influence $\M_i$ for reliable raters $i$ via 
the nuclear norm constraint (note that the other constraints are 
separable across rows). This makes sense because 
the nuclear norm is what causes us to pool global structure across 
raters (and thus potentially pool bad information). In order to 
limit this influence, the constraint on the 
nuclear norm is weaker than is typical by a factor of $\frac{2}{\epsilon}$; 
it is not clear to us whether this is actually necessary or due to a 
loose analysis.
% JS -- deleted b/c we didn't define C yet
%(Note that $M_{\good}^*$--i.e., $M^*$ restricted to the 
%reliable rows--has nuclear norm $\sqrt{\alpha\beta nm}$, since it is the 
%$\alpha n \times \beta m$ all-$1$s matrix padded by zeros; the constraint on $\|M\|_*$ must 
%be at least $\frac{1}{\alpha}$ times as large as this since the adversaries 
%could produce $\frac{1}{\alpha}$ permuted copies of $M_{\good}^*$.)

The constraint $\sum_j M_{ij} \leq \beta m$ is 
not typical in the literature. For instance, \citepm{chen2014improved} 
place no 
constraint on the sum of each row in $M$ (instead of recovering 
the $\beta$-quantile, they normalize $\A$ to lie in $[-1,1]^{m \times m}$ and recover 
the items with a positive rating).
Our row normalization constraint prevents an attack 
in which a spammer rates a random subset of items as high as possible and 
rates the remaining items as low as possible. If the actual set of 
high-quality items has density much smaller than $50\%$, then the 
spammer gains undue influence relative to honest raters that 
only rate e.g. $10\%$ of the items highly. Normalizing $M$ to 
have a fixed row sum prevents this; see Section~\ref{sec:adversary-examples} 
for details.
