\section{Assumptions and Approach}
\label{sec:assumptions}
\label{sec:approach}

We now state our assumptions more formally, state the general form 
of our results, and outline the key ingredients of the proof.
Recall that we obtain a matrix $\A \in [0,1]^{n \times m}$ by asking each 
rater $i \in [n]$ to rate $k$ items chosen at random from $[m]$.
Let $\good \subset [n]$ be the set of reliable raters, where $|\good| \geq \alpha n$.
Our main assumption is that the reliable raters make independent errors. We also 
need a way of measuring agreement between our ratings and those of the reliable 
raters; the amount of agreement will show up in our final error bounds --- if 
there is no agreement then the error will be high. We formalize both the 
independence assumption and the notion of agreement below.

\begin{assumption}[Independence]
\label{ass:independent}
The values $\A_{i,j}$ for $i \in \good$, $j \in [m]$, are 
jointly independent.
In addition, the values $\robs_j$ are jointly independent 
of each other as well as of $\A$ (including $\A_i$ for $i \not \in \good$).
\end{assumption}
Note that Assumption~\ref{ass:independent} allows the unreliable ratings to 
depend on the reliable ratings, allows arbitrary collusion among the unreliable 
raters, and even allows the unreliable raters to know the pattern of random 
assignments of the ratings. The only restriction on the unreliable raters is 
that they may not depend on \emph{our} randomness (i.e., the randomness in 
$\robs$). This can be ensured, for instance, if we wait to generate our 
personal randomness until after everyone else has submitted their ratings 
(assuming the adversaries don't gain access to our pseudorandom seed). We are 
also implicitly assuming that the adversaries cannot influence the set of 
items in a way that causes us or the reliable raters to accidentally rate a bad 
item highly or vice versa.

We next need a way of formalizing the idea that the reliable raters should 
agree with us. To this end, we let 
$\Aavg_{i,j} \eqdef \bE[\A_{i,j}]$ and $\ravg_j \eqdef \bE[\robs_j]$. We 
would like $\Aavg$ to be approximately increasing with $\ravg$:
\begin{definition}[Lipschitz raters]
\label{def:lipschitz}
We say that the reliable raters are \emph{$(L,\epsilon)$-Lipschitz} if their 
average ratings satisfy
\begin{equation}
\label{eq:lipschitz}
\ravg_j - \ravg_{j'} \leq \max\p{0, L \cdot (\Aavg_{i,j} - \Aavg_{i,j'}) + \epsilon}
\end{equation}
for all $i \in \good$ and all $j,j' \in [m]$.
\end{definition}
The $(L,\epsilon)$-Lipschitz property says that if we think that one item is 
substantially better than another item, then the reliable raters should think 
so as well. To give an example, suppose that all of our own ratings are binary 
($\ravg_j \in [0,1]$) and that each rating $\Aobs_{i,j}$ matches $\ravg_j$ 
with probability $\frac{3}{5}$. Then 
$\Aavg_{i,j} = \frac{2}{5} + \frac{1}{5}\ravg_j$, 
and hence the ratings are $(5,0)$-Lipschitz. 
In general, the Lipschitz property is fairly mild --- if the reliable ratings 
are not $(L,\epsilon)$-Lipschitz, then it is not clear that they should 
even be called reliable!

With Assumption~\ref{ass:independent} and Definition~\ref{def:lipschitz} 
in place, we are ready to state our main result. 

\begin{theorem}
\label{thm:main}
Suppose that Assumption~\ref{ass:independent} holds, and that each rater is 
assigned to rate $k$ items at random, that we rate $k'$ items at random, 
where $k \geq \Omega\p{\frac{1}{\beta\alpha^3\epsilon^4}\max\p{1, \frac{m}{n}}}$ 
and $k' \geq \Omega\p{\frac{\log(1/\alpha)}{\beta\epsilon^2}}$.
Then, if the reliable raters are $(L,\epsilon_0)$-Lipschitz, running 
Algorithms~\ref{alg:recover-M} and \ref{alg:recover-T} recovers a set 
$T$ satisfying 
\[ \frac{1}{\beta m} \p{\sum_{j \in T^*} \ravg_j - \sum_{j \in T} \ravg_j} 
\leq L \cdot \epsilon + \epsilon_0 \]
with probability $1-\delta$, where $T^*$ is the $\beta$-quantile of $\ravg$.
\end{theorem}

The proof of Theorem~\ref{thm:main} can be split into two parts: analyzing 
Algorithm~\ref{alg:recover-M}, and analyzing Algorithm~\ref{alg:recover-T}. 
At a high level, analyzing Algorithm~\ref{alg:recover-M} involves showing that 
the nuclear norm constraint in \eqref{eq:optimization-noisy} imparts sufficient 
noise robustness while not allowing the adversary too much influence over the 
reliable rows of $\M$. Analyzing~\ref{alg:recover-T} is far more straightforward, 
and requires only standard concentration inequalities and a standard randomized 
rounding idea (though the latter is perhaps not well-known, so we will explain 
it briefly in Section~\ref{sec:approach-T}).

\subsection{Recovering $\M$ (Algorithm~\ref{alg:recover-M})}
\label{sec:approach-M}

The goal of this section is to show that solving the optimization 
problem \eqref{eq:optimization-noisy} recovers a matrix $\M$ that 
approximates the $\beta$-quantile of $\ravg$ in the following sense:
\begin{proposition}
\label{prop:recover-M}
Under the conditions of Theorem~\ref{thm:main}, Algorithm~\ref{alg:recover-M} 
outputs a matrix $\M$ satisfying 
\[ \frac{1}{|\good|} \frac{1}{\beta m} \sum_{i \in \good} \sum_{j \in [m]} \M_{i,j}\ravg_j \geq \p{\frac{1}{\beta m} \sum_{j \in T^*} \ravg_j} - \p{L \cdot \epsilon + \epsilon_0}, \]
where $T^*$ is the $\beta$-quantile of $\ravg$.
\end{proposition}
Proving Proposition~\ref{prop:recover-M} involves two major steps: first, 
showing that the nuclear norm constraint in \eqref{eq:optimization-noisy} 
imparts noise-robustness, and second, that the constraint does not allow 
the adversaries to influence the reliable rows of $\M$ too much.

\paragraph{Part 1: noise-robustness.} Let $\Aa$ be the matrix satisfying 
$\Aa_{\good} = \frac{k}{m}\Aavg_{\good}$, $\Aa_{\bad} = \Aobs_{\bad}$. Here we 
let $A_{\good}$ denote the subset of rows of $A$ indexed by $\good$, and 
$A_{\bad}$ the remaining rows. The scaling $\frac{k}{m}$ is chosen so that 
$\bE[\Aobs_{\good}] = \Aa_{\good}$.
%where $\|\cdot\|_*$ denotes the nuclear norm. In the sequel, 
%we use $\sP$ to denote the constraint set in (\ref{eq:optimization-noisy}).

We also let $R_{ij} = \bI[i \in \good, j \in T^*]$.
Ideally, we would like to have $M_{\good} = R_{\good}$, i.e. $M$ matches $R$ on 
all the rows of $\good$. In light of this, 
we will let $\Mm$ be the solution to the following ``corrected'' program, which 
we don't have access to (since it involves knowledge of $\Aavg$ and $\good$), 
but which will be useful for analysis purposes:
%\begin{align}
%\label{eq:optimization-noiseless}
%\text{maximize } &\langle \Aa, M \rangle, \\
%\notag \text{ subject to } &M_{\good} = R_{\good}, \\
%\notag  &M_{ij} \geq 0 \,\,\, \forall i,j, \\
%\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
%\notag  &\max_j M_{ij} \leq \frac{1}{\beta m}\sum_j M_{ij} \,\,\, \forall i, \\
%\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}.
%\end{align}
\begin{align}
\label{eq:optimization-noiseless}
\text{maximize } &\langle \Aa, M \rangle, \\
\notag \text{ subject to } &M_{\good} = R_{\good}, \\
\notag  &0 \leq M_{ij} \leq 1 \,\,\, \forall i,j,  \\
%  &&\hskip-0.4in\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\sum_j M_{ij} \leq \beta m \,\,\, \forall i, \\
%  &&\hskip-0.4in\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}, \phantom{xxxxxxx}
\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\alpha\beta nm} \phantom{xxxxxxx}
\end{align}
Our goal is to show that $\M$ is ``close'' to $\Mm$. 
Following an idea of \citet{guedon2014community}, we have the following result:
\begin{lemma}
\label{lem:objective-bound}
Suppose that Assumption~\ref{ass:independent} holds and that 
$k = \Omega\p{\frac{1}{\alpha^3\beta\epsilon^4}\max\p{\frac{m}{n},1}}$. 
Then, the solution $\M$ to \eqref{eq:optimization-noisy} performs nearly as 
well as $\Mm$ under $B$:
\begin{equation}
\label{eq:objective-bound}
\langle \Aa, \M \rangle \geq \langle \Aa, \Mm \rangle - \epsilon \alpha\beta kn.
\end{equation}
\end{lemma}
Note that $\M$ is not necessarily feasible for \eqref{eq:optimization-noiseless}, 
because of the constraint $M_{\good} = R_{\good}$; Lemma~\ref{lem:objective-bound} 
merely asserts that $\M$ approximates $\Mm$ in objective value. The proof of 
Lemma~\ref{lem:objective-bound}, given in Section~\ref{sec:objective-bound-proof}, 
primarily involves establishing a 
\emph{uniform convergence result}; if we let $\sP$ denote the feasible set for 
\eqref{eq:optimization-noisy}, then we wish to show that 
$|\langle \A - B, M \rangle| \leq \frac{1}{2}\epsilon \alpha\beta kn$ for all 
$M \in \sP$. This would imply that the objectives of 
\eqref{eq:optimization-noisy} and \eqref{eq:optimization-noiseless} are 
essentially identical, and so optimizing one also optimizes the other.

Using the inequality $|\langle \A - B, M \rangle| \leq \|\A-B\|_{\op}\|M\|_*$, 
where $\|\cdot\|_{\op}$ denotes operator norm, it suffices to establish a matrix 
concentration inequality showing that $\|\A - B\|_{\op}$ is small. 
\todo{Greg, can you fill this in with the relevant matrix concentration 
result?}

\paragraph{Part 2: bounding the influence of adversaries.} 
Next, we need to show that the nuclear norm constraint does 
not give the adversaries too much influence over the de-noised program 
\eqref{eq:optimization-noiseless}; this is the most novel aspect 
of our argument.

Suppose that the constraint on $\|M\|_*$ were not present in 
\eqref{eq:optimization-noiseless}. Then the adversaries would have 
no influence on $\Mm_{\good}$, because all the remaining constraints 
in \eqref{eq:optimization-noiseless} are separable across rows. 
How can we quantify the effect of this nuclear norm constraint?
We exploit Lagrangian duality, which allows us to replace constraints 
with appropriate modifications to the objective function.

\input lagrangian-figure

In this instance, 
Lagrangian duality guarantees that \eqref{eq:optimization-noiseless} has the 
same solution as a modified problem where the constraint on $\|M\|_*$ is removed, 
and the objective is modified to $\langle B - Z, M \rangle$, for an appropriately 
chosen $Z$. In particular, $Z$ lies in the subgradient of 
$\mu \|M\|_*$ for some $\mu > 0$.

To gain some intuition, consider 
Figure~\ref{fig:lagrangian}. In \eqref{eq:optimization-noiseless}, we optimize 
$\langle B, M \rangle$ subject to $M_{\good} = R_{\good}$, as well as subject to 
the nuclear norm and other constraints; this yields the optimal value $M^*$. 
We know by Lemma~\ref{lem:objective-bound} that $\langle B, \M \rangle$ is almost 
as large as $\langle B, M^* \rangle$, and would like to conclude that 
$\M_{\good} \approx M^*_{\good}$. Since $B$ points to the right, we know that 
decreasing $M_{\good}$ will decrease $\langle B, M \rangle$; the problem is that 
we may then be able to increase $M_{\bad}$, which would increase 
$\langle B, M \rangle$ again. The solution is to find a Lagrange 
multiplier $Z$ that supports 
the constraint on $\|M\|_*$, in the sense that optimizing 
$\langle B-Z, M \rangle$ yields $M^*$ even without the nuclear norm constraint. 
Then, $M_{\good}$ and $M_{\bad}$ become separable and we conclude that 
$\langle B_{\good} - Z_{\good}, M_{\good} \rangle$ must be within $\epsilon$ of 
$\langle B_{\good} - Z_{\good}, M^*_{\good} \rangle$, which implies that 
$M_{\good} \approx M^*_{\good}$.

If we formalize this intuition and analyze $Z$ in detail, we obtain the 
following result:
\begin{lemma}
\label{lem:subgradient}
Suppose that $k = \Omega\p{\frac{1}{\alpha\beta\epsilon^2}\max\p{1,\frac{m}{n}}}$. 
Then there exists a matrix $Z$ with 
$\rank(Z) = 1$, $\|Z\|_F \leq \epsilon k\sqrt{\alpha\beta n/m}$ such that
\begin{align}
\langle \Aa_{\good} - Z_{\good}, \Mm_{\good} - M_{\good} \rangle &\leq \langle \Aa, \Mm - M \rangle \text{ for all $M \in \sP$}.
\end{align}
\end{lemma}
Essentially, Lemma~\ref{lem:subgradient} shows that any change in 
$\langle \Aa, M \rangle$ caused by changing $M_{\bad}$ can be upper-bounded 
by a small term $\langle Z_{\good}, \Mm_{\good} - M_{\good} \rangle$ that depends only 
on $M_{\good}$, thereby bounding the effect that the adversaries can have 
on $M_{\good}$. Lemma~\ref{lem:subgradient} is therefore the key 
technical tool powering our results, and is proved in 
Section~\ref{sec:subgradient-proof}.

\paragraph{Proof of Proposition~\ref{prop:recover-M}.}
In the remainder of this section, we will prove Proposition~\ref{prop:recover-M} 
from Lemmas~\ref{lem:objective-bound} and \ref{lem:subgradient}.
We start by plugging in $\M$ for $M$ in Lemma~\ref{lem:subgradient}. This yields
%\begin{align}
$\langle \Aa_{\good} - Z_{\good}, \Mm_{\good} - \M_{\good} \rangle \leq \langle \Aa, \Mm - \M \rangle \leq \epsilon \alpha\beta kn$
%\end{align}
by Lemma~\ref{lem:objective-bound}.
On the other hand, we have 
\begin{align}
|\langle Z_{\good}, \Mm_{\good} - \M_{\good} \rangle| &\leq \|Z_{\good}\|_F\|\Mm_{\good} - \M_{\good}\|_F \\
 &\leq \epsilon\sqrt{\alpha\beta nk/m} \sqrt{\|\Mm_{\good} - \M_{\good}\|_{1}\|\Mm_{\good} - \M_{\good}\|_{\infty}} \\
 &\leq \epsilon\sqrt{\alpha\beta nk/m} \sqrt{2\alpha\beta mn} = \sqrt{2}\epsilon\alpha\beta kn.
\end{align}
Putting these together, we obtain
%\begin{align}
$\langle \Aa_{\good}, \Mm_{\good} - \M_{\good} \rangle \leq (1+\sqrt{2})\epsilon \alpha\beta kn$.
%\end{align}
Expanding $\langle \Aa_{\good}, \Mm_{\good} - \M_{\good} \rangle$ as 
$\frac{k}{m}\sum_{i \in \good}\p{\sum_{j \in [m]} (R_{ij} - \M_{ij})\Aavg_{ij}}$,
we obtain 
\[ \frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij}\Aavg_{ij}\Big) \leq (1+\sqrt{2})\epsilon. \]
Finally, by the $(L,\epsilon_0)$-Lipschitz condition, we have (see \ref{sec:lipschitz-details} for details)
\begin{align}
\label{eq:lipschitz-usage}
\frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \ravg_j - \sum_{j \in [m]} \M_{ij}\ravg_j\Big) 
&\leq L \cdot \frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij}\Aavg_{ij}\Big) + \epsilon_0 \\
&\leq L \cdot (1+\sqrt{2})\epsilon + \epsilon_0.
\end{align}
The desired result then follows after scaling down $\epsilon$ 
by a factor of $1+\sqrt{2}$.

\subsection{Recovering $T$ (Algorithm~\ref{alg:recover-T})}
\label{sec:approach-T}
\label{sec:rounding}

In this section we show that if $\M$ satisfies the conclusion of 
Proposition~\ref{prop:recover-M}, then Algorithm~\ref{alg:recover-T} 
recovers a set $T$ that approximates $T^*$ well. Formally, we show 
the following:
\begin{proposition}
\label{prop:recover-T}
Suppose that the output $\M$ of Algorithm~\ref{alg:recover-M} satisfies 
$\frac{1}{|\good|} \frac{1}{\beta m}\sum_{i \in \good}\sum_{j \in [m]} \M_{ij}\ravg_j \geq \p{\frac{1}{\beta m} \sum_{j \in T^*} \ravg_j} - \epsilon_1$. Then, 
assuming $k_0 \geq \Omega\p{\frac{\log(1/\alpha)}{\beta\epsilon^2}}$, 
Algorithm~\ref{alg:recover-T} outputs a set $T$ satisfying 
\begin{equation}
\label{eq:recover-T}
\frac{1}{\beta m}\sum_{j \in T} \ravg_j \geq \p{\frac{1}{\beta m}\sum_{j \in T^*} \ravg_j} - (\epsilon_1 + \epsilon).
\end{equation}
\end{proposition}
The validity of this procedure hinges on two results. First, we need to establish 
a concentration bound showing that $\sum_j \M_{ij}\robs_j$ is close to 
$\frac{k_0}{m}\sum_j \M_{ij}\rtrue_j$ for all $i \in \good$, which implies that 
the $\goodfrac n$ best rows of $\M$ according to $\robs$ also look good 
according to $\robs$. This is captured in the following lemma:
\begin{lemma}
\label{lem:robs-rtrue}
Suppose that $\robs$ satisfies Assumption~\ref{ass:independent} and that 
$k_0 \geq \Omega\p{\frac{\log(1/\failprob\alpha)}{\beta\epsilon^2}}$. 
Then, with probability $1-\delta$, we have
\begin{equation}
\label{eq:robs-rtrue}
\frac{1}{\goodfrac n} \sum_{i \in \goodobs} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) \geq \frac{1}{|\good|} \sum_{i \in \good} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) - \frac{\epsilon}{3}.
\end{equation}
\end{lemma}
See Section~\ref{sec:concentration-proof} for a proof.
The idea is to establish a uniform bound showing that 
$\sum_{i \in S} \sum_{j \in [m]} \M_{ij}(\robs_j - \rtrue_j)$ is small for any 
set of $\goodfrac n$ rows $S$, and hence that greedily taking the $\goodfrac n$ 
best rows according to $\robs$ is almost as good as taking the $\goodfrac n$ 
best rows according to $\rtrue$. Our analysis follows that of 
\citet{todo}, improving over a na\"{i}ve union bound by exploiting power mean 
inequalities for the cumulant function. \todo{Greg can you fill in this 
citation?}

Once we have recovered a set $\goodapprox$ of good rows, we need to turn this 
into an actual set $T \subset [m]$. To this end, first let $T_0 \in [0,1]^m$ 
be defined as $T_0 \eqdef \frac{1}{|\goodapprox|} \sum_{i \in \goodapprox} \M_i$. 
We will use randomized rounding to obtain a vector $T \in \{0,1\}^m$ such that 
$\bE[T_0] = T$; $T$ is then the indicator function of our desired set.
Our rounding procedure is given in Algorithm~\ref{alg:round}; the following 
lemma, proved in \ref{sec:rounding-proof}, asserts its correctness:
\begin{lemma}
\label{lem:rounding}
The output $T$ of Algorithm~\ref{alg:round} satisfies $\bE[T] = T_0$, 
$\|T\|_0 \leq \beta m$.
\end{lemma}

\input round

Recall that in Algorithm~\ref{alg:recover-T}, we accept $T$ if 
$\langle T, \robs \rangle \geq \langle T_0, \robs \rangle - \frac{1}{3}\epsilon\beta k$. Since $\langle T, \robs \rangle \in [0,k]$ almost surely, and 
$\bE[\langle T, \robs \rangle] = \langle T_0, \robs \rangle$, Markov's inequality 
implies that we accept with probability at least $\frac{\beta \epsilon}{3}$ on 
each round, and so with probability $1-\delta$ have at most 
$\frac{3\log(1/\delta)}{\epsilon\beta}$ rounds. By a union bound, 
$|\langle T, \robs - \frac{k}{m}\ravg \rangle| \leq \frac{\epsilon}{3}$ is 
small for all of the rounds, implying that the accepted set $T$ satisfies 
\begin{align}
\sum_{j \in T} \ravg_j &\geq \frac{m}{k}\sum_{j \in T} \robs_j - \frac{\epsilon}{3}\beta m \\
 &\geq \frac{m}{k}\sum_{j \in [m]} (T_0)_j \robs_j - \frac{2\epsilon}{3}\beta m \geq \sum_{j \in T^*} \ravg_j - (\epsilon_1+\epsilon),
\end{align}
which is what we wanted to show in Proposition~\ref{prop:recover-T}. We refer 
the reader to Section~\ref{sec:recover-T-proof} for a more detailed proof.
