\section{Approach}

Given an $\Aobs$ satisfying Assumption~\ref{ass:A}, we solve the following 
nuclear-norm optimization problem:

\begin{align}
\label{eq:optimization-noisy}
\text{maximize } &\langle \A, M \rangle, \\
\notag \text{ subject to } &M_{ij} \geq 0 \,\,\, \forall i,j, \\
\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\max_j M_{ij} \leq \frac{1}{\beta m}\sum_j M_{ij} \,\,\, \forall i, \\
\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}},
\end{align}
where $\|\cdot\|_*$ denotes the nuclear norm. In the sequel, 
we use $\sP$ to denote the constraint set in (\ref{eq:optimization-noisy}).

To gain some intuition, we will 
discuss each of the terms in \eqref{eq:optimization-noisy}. The objective 
$\langle \Aobs, M \rangle$ asks that $M$ match the noisy ratings 
$\Aobs$, while the nuclear norm constraint acts as a regularizer which will 
ensure that matching the noisy ratings $\Aobs$ implies that we also match the true 
ratings $\Aavg$. The constraints $M_{ij} \geq 0$, $\|M_i\|_1 \leq 1$, 
$\|M_i\|_{\infty} \leq \frac{1}{\beta m} \|M_i\|_1$ mean that each row of 
$M$ is a probability distribution supported on at least $\beta m$ elements. 

Constraining each row in $\ell^1$-norm is important in the adversarial setting, 
and contrasts with e.g. \citet{todo}, who instead constrain each row in 
$\ell^{\infty}$-norm. The $\ell^1$ constraint ensures that each row gets the 
same amount of ``weight'', and prevents adversarial strategies based on assigning 
very high ratings to a random $50\%$ of the items, which would otherwise 
increase the amount of work necessary by polynomial factors in $\frac{1}{\beta}$.

\paragraph{Analysis strategy.}
At a high level our strategy follows that of several other authors 
\citep{todo}, and proceeds by (1) comparing \eqref{eq:optimization-noisy} to a 
de-noised version, and (2) showing that the de-noised program recovers the 
information we want. However, the adversarial setting necessitates some 
differences: we only de-noise the part of $\Aobs$ corresponding to $\good$, 
and analyzing the de-noised program (which is usually straightforward) requires 
some care to ensure that the adversary cannot compromise the solution.

To make our approach more precise, let $\Aa$ be the matrix that is equal to 
$\Aavg_{\sC}$ for the rows in $\sC$, 
and equal to $\Aobs$ for the rows outside of $\sC$. Also let 
$R_{ij} = \frac{1}{\beta m}\bI[i \in \sC, j \in Q_{\beta}]$. 
Ideally, we would like to have $M_{\sC} = R_{\sC}$, i.e. $M$ matches $R$ on 
all the rows of $\sC$. In light of this, 
we will let $\Mm$ be the solution to the following ``corrected'' program, which 
we don't have access to (since it involves knowledge of $\Aavg$ and $\sC$), 
but which will be useful for analysis purposes:
\begin{align}
\label{eq:optimization-noiseless}
\text{maximize } &\langle \Aa, M \rangle, \\
\notag \text{ subject to } &M_{\sC} = R_{\sC}, \\
\notag  &M_{ij} \geq 0 \,\,\, \forall i,j, \\
\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\max_j M_{ij} \leq \frac{1}{\beta m}\sum_j M_{ij} \,\,\, \forall i, \\
\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}.
\end{align}

Our goal is to show that $\M$ is ``close'' to $M^*$ and hence gives information 
about $\sC$ and $Q_{\beta}$. 
We formalize this in the following theorem:
\begin{theorem}
\label{thm:main}
Let $\fac = \Omega\p{\frac{1}{\alpha^2\beta\epsilon^4}\max\p{\frac{m}{\alpha n},1}}$. Then, it is the case that 
\[ \frac{1}{|\good|} \sum_{i \in \good} \p{\frac{1}{\beta m} \sum_{j \in Q_{\beta}} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij} \Aavg_{ij}} \leq \epsilon. \]
%$\|\M_i - \pi^*\|_1 \leq \epsilon$, 
%where $\pi^*_j = \frac{1}{|\sD|}$ if $j \in \sD$, and $0$ otherwise.
\end{theorem}
Theorem~\ref{thm:main} says that on average across the good rows, we recover 
a probability distribution whose expected rating is within $\epsilon$ of 
the rating of the $\beta$-quantile. 

The two key components in proving Theorem~\ref{thm:main} are 
given as Proposition~\ref{prop:objective-bound} and 
Proposition~\ref{prop:subgradient} below. First, we show 
(Proposition~\ref{prop:objective-bound}) that $\M$ is an approximate optimizer 
of \ref{eq:optimization-noiseless}, which is true because 
$\langle \Aobs, M \rangle$ and $\langle \Aa, M \rangle$ are close together for 
all $M \in \sP$.
\begin{proposition}
\label{prop:objective-bound}
Let $\fac = \Omega\p{\frac{1}{\alpha^2\beta\epsilon^4}\max\p{\frac{m}{\alpha n},1}}$. For $\M$ and $\Mm$ as defined above, we have
\begin{equation}
\label{eq:objective-bound}
\langle \Aa, \M \rangle \geq \langle \Aa, \Mm \rangle - \epsilon \alpha n.
\end{equation}
\end{proposition}

Next, we need to show that the nuclear norm constraint does 
not create too much bias in the de-noised program 
\eqref{eq:optimization-noiseless}. Note that this is the \emph{only} constraint 
that affects more than one row in \eqref{eq:optimization-noiseless}, and without 
it it would be apparent that every $M^*_i$, for $i \in \sC$, would be equal to the 
$\beta$-quantile $Q_{\beta}$. Our goal is to show that the nuclear norm 
constraint doesn't create too much dependence between the rows, which we can 
formalize by bounding the Lagrangian of \eqref{eq:optimization-noiseless}. 
Doing so leads to:

\begin{proposition}
\label{prop:subgradient}
Let $\fac = \Omega\p{\frac{1}{\alpha\beta\epsilon^2}\frac{m}{n}}$. Then there exists a matrix $Z$ with 
$\rank(Z) = 1$, $\|Z\|_F \leq \epsilon \sqrt{\alpha\beta mn}$ such that
\begin{align}
\langle \Aa_{\sC} - Z_{\sC}, \Mm_{\sC} - M_{\sC} \rangle &\leq \langle \Aa, \Mm - M \rangle \text{ for all $M \in \sP$}.
\end{align}
\end{proposition}
Essentially, Proposition~\ref{prop:subgradient} shows that any change in 
$\langle \Aa, M \rangle$ caused by changing $M_{\bad}$ can be upper-bounded 
by a small term $\langle Z_{\sC}, \Mm_{\sC} - M_{\sC} \rangle$ that depends only 
on $M_{\good}$, thereby bounding the effect that the adversaries can have 
on $M_{\sC}$. 

In the remainder of this section, we will prove Theorem~\ref{thm:main} from Propositions~\ref{prop:objective-bound} 
and \ref{prop:subgradient}. The propositions themselves will be proved in later sections.

\begin{proof}[Proof of Theorem~\ref{thm:main}]
We start by plugging in $\M$ for $M$ in Proposition~\ref{prop:subgradient}. This yields
\begin{align}
\langle \Aa_{\sC} + Z_{\sC}, \Mm_{\sC} - \M_{\sC} \rangle &\leq \langle \Aa, \Mm - \M \rangle \\
 &\leq \epsilon \alpha n
\end{align}
by Proposition~\ref{prop:objective-bound}.
On the other hand, we have 
\begin{align}
|\langle Z_{\sC}, \Mm_{\sC} - \M_{\sC} \rangle| &\leq \|Z_{\sC}\|_F\|\Mm_{\sC} - \M_{\sC}\|_F \\
 &\leq \epsilon\sqrt{\alpha\beta nm} \sqrt{\sum_{i \in \sC} \|\Mm_i - \M_i\|_2^2} \\
 &\leq \epsilon\sqrt{\alpha\beta nm} \sqrt{\sum_{i \in \sC} \|\Mm_i - \M_i\|_{\infty}\|\Mm_i - \M_i\|_1} \\
 &=    \epsilon\sqrt{\alpha\beta nm} \sqrt{\frac{\|\Mm_{\sC}-\M_{\sC}\|_1}{\beta m}} \\
 &\leq \epsilon\sqrt{\alpha\beta nm} \sqrt{2\alpha n/\beta m} \\
 &= \sqrt{2}\epsilon\alpha n.
\end{align}
Putting these together, we obtain
\begin{align}
\langle \Aa_{\sC}, \Mm_{\sC} - \M_{\sC} \rangle &\leq (1+\sqrt{2})\epsilon \alpha n.
\end{align}
Expanding $\langle \Aa_{\sC}, \Mm_{\sC} - \M_{\sC} \rangle$ as 
$\sum_{i \in \sC}\p{\sum_{j \in [m]} (R_{ij} - \M_{ij})\Aavg_{ij}}$,
we obtain 
\[ \frac{1}{|\sC|}\sum_{i \in \sC}\p{\frac{1}{\beta m}\sum_{j \in Q_{\beta}} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij}\Aavg_{ij}} \leq (1+\sqrt{2})\epsilon. \]
The desired result then follows after scaling down $\epsilon$ 
by a factor of $1+\sqrt{2}$.
\end{proof}

\section{Recovering $\rtrue$}

\todo{talk about greedy algorithm}

\section{Proof of Proposition~\ref{prop:objective-bound}}
By H\"{o}lder's inequality, we have 
\begin{align}
|\langle \A-\Aa, M \rangle| &\leq \|\A-\Aa\|_{\op}\|M\|_* \\
 &= \|\A_{\sC}-\Aavg_{\sC}\|_{\op}\|M\|_* \\
 &\leq \frac{m}{\sqrt{\fac}}\max\p{1, \sqrt{\frac{\alpha n}{m}}} \cdot \frac{2}{\epsilon\alpha}\sqrt{\frac{\alpha n}{\beta m}} \\
 &= \frac{2\sqrt{mn}}{\epsilon\sqrt{\alpha\beta\fac}}\max\p{\sqrt{\frac{\alpha n}{m}}, 1}. %\frac{\alpha n}{m}}.
\end{align}
Setting $\fac = \frac{16}{\alpha^2\beta\epsilon^4}\max\p{\frac{m}{\alpha n}, 1}$, 
we thus have
\begin{align}
|\langle \A-\Aa, M \rangle| &\leq \frac{1}{2}\epsilon \alpha n.
\end{align}
We then see that
\begin{align}
\langle \Aa, \M \rangle  &\geq \langle \A, \M \rangle - \frac{1}{2}\epsilon \alpha n \\
 &\geq \langle \A, \Mm \rangle - \frac{1}{2} \epsilon \alpha n \text{ (since $\M$ is optimal for $\A$)} \\
 &\geq \langle \Aa, \Mm \rangle - \epsilon \alpha n,
\end{align}
which completes the proof.

%%% TODO --- this is good intuition, should include it somewhere
%%% At a high level, our strategy is the following --- since $\Mm$ is already optimized 
%%% on the rows outside of $\sC$, we should (``morally'', though not in actuality) have 
%%% $\langle \Aa_{\neg \sC}, \Mm_{\neg \sC} \rangle \geq \langle \Aa_{\neg \sC}, \M_{\neg \sC} \rangle$. 
%%% 
%%% Looking back at the rows inside $\sC$, we must therefore have 
%%% $\langle \Aa_{\sC}, \M_{\sC} \rangle \geq \langle \Aa_{\sC}, \Mm_{\sC} \rangle - \epsilon \alpha n$, 
%%% which is exactly the statement \eqref{eq:M-bound} with $(1+\sqrt{2})\epsilon$ replaced by $\epsilon$.
%%% 
%%% The problem with the above argument is that, even though $\Mm$ is optimized on $\neg \sC$, the rows in 
%%% $\sC$ and $\neg \sC$ are coupled by the joint constraint $\|M\|_* \leq \frac{2}{\epsilon\alpha}$. We 
%%% will decouple the rows by taking the subgradient of the Lagrangian at $\Mm$, which yields an upper bound 
%%% for our optimization problem. By doing this we obtain:

\section{Proof of Proposition~\ref{prop:subgradient}}
Let $\sP_0$ be the superset of $\sP$ where we have removed the 
nuclear norm constraint. By Lagrangian duality we 
know that there is some $\mu$ such that maximizing 
$\langle \Aa, M \rangle$ over $\sP \cap \{M_{\sC} = R_{\sC}\}$ 
is equivalent to maximizing $f_\mu(M) \eqdef \langle \Aa, M \rangle + \mu\p{\frac{2}{\epsilon\alpha}\sqrt{\frac{\alpha n}{\beta m}} - \|M\|_*}$ over 
$\sP_0 \cap \{M_{\sC} = R_{\sC}\}$. 

We start by bounding $\mu$. We claim that $\mu \leq \epsilon \sqrt{\alpha\beta nm}$. 
To show this, first note that for any $M \in \sP_0$, we have 
\begin{align}
\langle \Aa, M \rangle &\leq \langle \Aavg, M \rangle + \langle \Aa - \Aavg, M \rangle \\
 &\leq n + \|\Aa - \Aavg\|_{\op}\|M\|_* \\
 &\leq n + \frac{m}{\sqrt{\fac}}\|M\|_* \\
 &\leq n + \frac{\epsilon\sqrt{\alpha\beta nm}}{2}\|M\|_*,
\end{align}
where in the last step we used the assumption $\fac = \Omega\p{\frac{1}{\alpha\beta\epsilon^2}\frac{m}{n}}$.

Now, suppose that we take $\mu_0 = \epsilon \sqrt{\alpha\beta nm}$ and optimize $\langle \Aa, M \rangle - \mu_0\|M\|_*$ over 
$\sP_0 \cap \{M_{\sC} = R_{\sC}\}$. Since $\langle \Aa, M \rangle - \mu_0\|M\|_* \leq n - \frac{\epsilon \sqrt{\alpha\beta nm}}{2}\|M\|_*$, 
any $M$ with $\|M\|_* > \frac{2}{\epsilon\alpha}\sqrt{\frac{\alpha n}{\beta m}}$ cannot possibly be optimal, since the solution $M = 0$ would 
be better. Hence, $\mu_0$ is a large enough Lagrange multiplier to ensure that $M \in \sP$, and so 
$\mu \leq \mu_0 = \epsilon \sqrt{\alpha\beta nm}$, as claimed.

We next characterize the subgradient of $f_{\mu}$ at $M = \Mm$.
Define the projection matrix $P$ as
\[ P_{i,i'} = \left\{ \begin{array}{ccl} \frac{1}{|\sC|} & : & i, i' \in \sC \\ \delta_{i,i'} & : \text{else} \end{array} \right.. \]
Thus $PM = M$ if and only if all rows in $\sC$ are equal to each other.
In particular, $PM = M$ whenever $M_{\sC} = R_{\sC}$. Now, since $\Mm$ is the maximum 
of $f_{\mu}(M)$ over all $M \in \sP_0 \cap \{M_{\sC} = R_{\sC}\}$, there must be some 
$B \in \partial f(\Mm)$ such that $\langle B, M - \Mm \rangle \leq 0$ for all $M \in \sP_0 \cap \{M_{\sC} = R_{\sC}\}$.
\begin{lemma}
\label{lem:subgradient}
Suppose that $B \in \partial f(\Mm)$ satisfies $\langle B, M - \Mm \rangle \leq 0$ for all $M \in \sP_0 \cap \{M_{\sC} = R_{\sC}\}$. 
Then, $PB$ satisfies the same property, and lies in $\partial f(\Mm)$ as well.
\end{lemma}
We can further note (by differentiating $f_{\mu}$) that 
$B = \Aa - \mu Z_0$, where $\|Z_0\|_{\op} \leq 1$. Hence 
$PB = P\Aa - \mu PZ_0 = \Aa - \mu PZ_0$. Let $r(M)$ denote the 
matrix where $M_{\sC}$ is replaced with $R_{\sC}$ (so $r(M) \in \sP_0 \cap \{R_{\sC} = M_{\sC}\}$ 
whenever $M \in \sP_0$). The rest of the proof is basically algebra:
\begin{align}
\langle \Aa, M - \Mm \rangle &\stackrel{(i)}{\leq} f_{\mu}(M) - f_{\mu}(\Mm) \\
 &\stackrel{(ii)}{\leq} \langle \Aa - \mu PZ_0, M - \Mm \rangle \\
 &= \langle \Aa - \mu PZ_0, M - r(M) \rangle + \langle \Aa - \mu PZ_0, r(M) - \Mm \rangle \\
 &\stackrel{(iii)}{\leq} \langle \Aa - \mu PZ_0, M - r(M) \rangle \\
 &= \langle \Aa_{\sC} - \mu (PZ_0)_{\sC}, M_{\sC} - r(M)_{\sC} \rangle \\
 &= \langle \Aa_{\sC} - \mu (PZ_0)_{\sC}, M_{\sC} - \Mm_{\sC} \rangle,
\end{align}
where (i) is by complementary slackness (either $\mu = 0$ or $\|\Mm\|_* = \frac{2}{\alpha\epsilon}$); 
(ii) is concavity of $f_{\mu}$, and the fact that $\Aa - \mu PZ_0$ is a subgradient; and 
(iii) is the property from Lemma~\ref{lem:subgradient} ($\langle \Aa - \mu PZ_0, r(M) - \Mm \rangle \leq 0$ since 
$r(M) \in \sP_0$).

To finish, we will take $Z = \mu (PZ_0)_{\sC}$. We note that
$\|Z\|_{\op} = \|\mu (PZ)_{\sC}\|_{\op} \leq \mu \|PZ\|_{\op} \leq \mu \|Z\|_{\op} \leq \mu$.
Moreover, $Z$ has rank $1$ and so $\|Z\|_F = \|Z\|_{\op} \leq \mu \leq \epsilon\sqrt{\alpha\beta nm}$, as was to be shown.

\subsection{Proof of Lemma~\ref{lem:subgradient}}
First, since $PM = M$ for all $M \in \sP_0 \cap \{M_{\sC} = R_{\sC}\}$, and $PM^* = M^*$, 
it is clear that $\langle PB, M-M^* \rangle = \langle B, P(M-M^*) \rangle = \langle B, M-M^* \rangle \leq 0$, 
so we need only show that $PB$ is still a subgradient of $f_{\mu}$. Indeed, we have (for any $M$)
\begin{align}
\langle PB, M-M^* \rangle &= \langle B, PM - M^* \rangle \\
 &\stackrel{(i)}{\geq} f_{\mu}(PM) - f_{\mu}(M^*) \\
 &= \langle A^*, PM \rangle - \mu \|PM\|_* - f_{\mu}(M^*) \\
 &= \langle A^*, M \rangle - \mu \|PM\|_* - f_{\mu}(M^*) \\
 &\stackrel{(ii)}{\geq} \langle A^*, M \rangle - \mu \|M\|_* - f_{\mu}(M^*) \\
 &= f_{\mu}(M) - f_{\mu}(M^*),
\end{align}
where (i) is because $B \in \partial f_{\mu}(M^*)$, and (ii) is because projecting 
decreases the nuclear norm. Since the inequality $\langle PB, M-M^* \rangle \geq f_{\mu}(M) - f_{\mu}(M^*)$ 
is the defining property of $\partial f_{\mu}(M^*)$, the proof is complete.

%\bibliographystyle{plainnat}
%\bibliography{references}

