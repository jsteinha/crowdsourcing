\section{Assumptions and Approach}
\label{sec:assumptions}
\label{sec:approach}

We now state our assumptions more formally, state the general form 
of our results, and outline the key ingredients of the proof.
Recall that we obtain a matrix $\A \in [0,1]^{n \times m}$ by asking each 
rater $i \in [n]$ to rate $k$ items chosen at random from $[m]$.
Let $\good \subset [n]$ be the set of reliable raters, where $|\good| \geq \alpha n$.
Our main assumption is that the reliable raters make independent errors. We also 
need a way of measuring agreement between our ratings and those of the reliable 
raters; the amount of agreement will show up in our final error bounds --- if 
there is no agreement then the error will be high. We formalize both the 
independence assumption and the notion of agreement below.

\begin{assumption}[Independence]
\label{ass:independent}
The values $\A_{i,j}$ for $i \in \good$, $j \in [m]$, are 
jointly independent.
In addition, the values $\robs_j$ are jointly independent 
of each other as well as of $\A$ (including $\A_i$ for $i \not \in \good$).
\end{assumption}
Note that Assumption~\ref{ass:independent} allows the unreliable ratings to 
depend on the reliable ratings, allows arbitrary collusion among the unreliable 
raters, and even allows the unreliable raters to know the pattern of random 
assignments of the ratings. The only restriction on the unreliable raters is 
that they may not depend on \emph{our} randomness (i.e., the randomness in 
$\robs$). This can be ensured, for instance, if we wait to generate our 
personal randomness until after everyone else has submitted their ratings 
(assuming the adversaries don't gain access to our pseudorandom seed). We are 
also implicitly assuming that the adversaries cannot influence the set of 
items in a way that causes us or the reliable raters to accidentally rate a bad 
item highly or vice versa.

We next need a way of formalizing the idea that the reliable raters should 
agree with us. To this end, we let 
$\Aavg_{i,j} \eqdef \bE[\A_{i,j}]$ and $\ravg_j \eqdef \bE[\robs_j]$. We 
would like $\Aavg$ to be approximately increasing with $\ravg$:
\begin{definition}[Lipschitz raters]
\label{def:lipschitz}
We say that the reliable raters are \emph{$(L,\epsilon)$-Lipschitz} if their 
average ratings satisfy
\begin{equation}
\label{eq:lipschitz}
\ravg_j - \ravg_{j'} \leq \max\p{0, L \cdot (\Aavg_{i,j} - \Aavg_{i,j'}) + \epsilon}
\end{equation}
for all $i \in \good$ and all $j,j' \in [m]$.
\end{definition}
The $(L,\epsilon)$-Lipschitz property says that if we think that one item is 
substantially better than another item, then the reliable raters should think 
so as well. To give an example, suppose that all of our own ratings are binary 
($\ravg_j \in [0,1]$) and that each rating $\Aobs_{i,j}$ matches $\ravg_j$ 
with probability $\frac{3}{5}$. Then 
$\Aavg_{i,j} = \frac{2}{5} + \frac{1}{5}\ravg_j$, 
and hence the ratings are $(5,0)$-Lipschitz. 
In general, the Lipschitz property is fairly mild --- if the reliable ratings 
are not $(L,\epsilon)$-Lipschitz, then it is not clear that they should 
even be called reliable!

With Assumption~\ref{ass:independent} and Definition~\ref{def:lipschitz} 
in place, we are ready to state our main result. 

\begin{theorem}
\label{thm:main}
Suppose that Assumption~\ref{ass:independent} holds, and that each rater is 
assigned to rate $k$ items at random, that we rate $k'$ items at random, 
where $k \geq \Omega\p{\frac{1}{\beta\alpha^3\epsilon^4}\max\p{1, \frac{m}{n}}}$ 
and $k' \geq \Omega\p{\frac{\log(1/\alpha)}{\beta\epsilon^2}}$.
Then, if the reliable raters are $(L,\epsilon_0)$-Lipschitz, running 
Algorithms~\ref{alg:recover-M} and \ref{alg:recover-T} recovers a set 
$T$ satisfying 
\[ \frac{1}{\beta m} \p{\sum_{j \in T^*} \ravg_j - \sum_{j \in T} \ravg_j} 
\leq L \cdot \epsilon + \epsilon_0 \]
with probability $1-\delta$, where $T^*$ is the $\beta$-quantile of $\ravg$.
\end{theorem}

The proof of Theorem~\ref{thm:main} can be split into two parts: analyzing 
Algorithm~\ref{alg:recover-M}, and analyzing Algorithm~\ref{alg:recover-T}. 
At a high level, analyzing Algorithm~\ref{alg:recover-M} involves showing that 
the nuclear norm constraint in \eqref{eq:optimization-noisy} imparts sufficient 
noise robustness while not allowing the adversary too much influence over the 
reliable rows of $\M$. Analyzing~\ref{alg:recover-T} is far more straightforward, 
and requires only standard concentration inequalities and a standard randomized 
rounding idea (though the latter is perhaps not well-known, so we will explain 
it briefly in Section~\ref{sec:approach-T}).

\subsection{Recovering $\M$ (Algorithm~\ref{alg:recover-M})}
\label{sec:approach-M}

The goal of this section is to show that solving the optimization 
problem \eqref{eq:optimization-noisy} recovers a matrix $\M$ that 
approximates the $\beta$-quantile of $\ravg$ in the following sense:
\begin{proposition}
\label{prop:recover-M}
Under the conditions of Theorem~\ref{thm:main}, Algorithm~\ref{alg:recover-M} 
outputs a matrix $\M$ satisfying 
\[ \frac{1}{|\good|} \frac{1}{\beta m} \sum_{i \in \good} \sum_{j \in [m]} \M_{i,j}\ravg_j \geq \p{\frac{1}{\beta m} \sum_{j \in T^*} \ravg_j} - \p{L \cdot \epsilon + \epsilon_0}, \]
where $T^*$ is the $\beta$-quantile of $\ravg$.
\end{proposition}
Proving Proposition~\ref{prop:recover-M} involves two major steps: first, 
showing that the nuclear norm constraint in \eqref{eq:optimization-noisy} 
imparts noise-robustness, and second, that the constraint does not allow 
the adversaries to influence the reliable rows of $\M$ too much.

\paragraph{Part 1: noise-robustness.} Let $\Aa$ be the matrix satisfying 
$\Aa_{\good} = \frac{k}{n}\Aavg_{\good}$, $\Aa_{\bad} = \Aobs_{\bad}$. Here we 
let $A_{\good}$ denote the subset of rows of $A$ indexed by $\good$, and 
$A_{\bad}$ the remaining rows. The scaling $\frac{k}{n}$ is chosen so that 
$\bE[\Aobs_{\good}] = \Aa_{\good}$.
%where $\|\cdot\|_*$ denotes the nuclear norm. In the sequel, 
%we use $\sP$ to denote the constraint set in (\ref{eq:optimization-noisy}).

We also let $R_{ij} = \bI[i \in \good, j \in T^*]$.
Ideally, we would like to have $M_{\good} = R_{\good}$, i.e. $M$ matches $R$ on 
all the rows of $\good$. In light of this, 
we will let $\Mm$ be the solution to the following ``corrected'' program, which 
we don't have access to (since it involves knowledge of $\Aavg$ and $\good$), 
but which will be useful for analysis purposes:
%\begin{align}
%\label{eq:optimization-noiseless}
%\text{maximize } &\langle \Aa, M \rangle, \\
%\notag \text{ subject to } &M_{\good} = R_{\good}, \\
%\notag  &M_{ij} \geq 0 \,\,\, \forall i,j, \\
%\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
%\notag  &\max_j M_{ij} \leq \frac{1}{\beta m}\sum_j M_{ij} \,\,\, \forall i, \\
%\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}.
%\end{align}
\begin{align}
\label{eq:optimization-noiseless}
\text{maximize } &\langle \Aa, M \rangle, \\
\notag \text{ subject to } &M_{\good} = R_{\good}, \\
\notag  &0 \leq M_{ij} \leq 1 \,\,\, \forall i,j,  \\
%  &&\hskip-0.4in\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\sum_j M_{ij} \leq \beta m \,\,\, \forall i, \\
%  &&\hskip-0.4in\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}, \phantom{xxxxxxx}
\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\alpha\beta nm} \phantom{xxxxxxx}
\end{align}
Our goal is to show that $\M$ is ``close'' to $\Mm$. 
Following an idea of \citet{guedon2014community}, we have the following result:
\begin{lemma}
\label{lem:objective-bound}
Suppose that Assumption~\ref{ass:independent} holds and that 
$k = \Omega\p{\frac{1}{\alpha^3\beta\epsilon^4}\max\p{\frac{m}{n},1}}$. 
Then, the solution $\M$ to \eqref{eq:optimization-noisy} performs nearly as 
well as $\Mm$ under $B$:
\begin{equation}
\label{eq:objective-bound}
\langle \Aa, \M \rangle \geq \langle \Aa, \Mm \rangle - \epsilon \alpha\beta km.
\end{equation}
\end{lemma}
Note that $\M$ is not necessarily feasible for \eqref{eq:optimization-noiseless}, 
because of the constraint $M_{\good} = R_{\good}$; Lemma~\ref{lem:objective-bound} 
merely asserts that $\M$ approximates $\Mm$ in objective value. The proof of 
Lemma~\ref{lem:objective-bound}, given in Section~\ref{sec:objective-bound-proof}, 
primarily involves establishing a 
\emph{uniform convergence result}; if we let $\sP$ denote the feasible set for 
\eqref{eq:optimization-noisy}, then we wish to show that 
$|\langle \A - B, M \rangle| \leq \frac{1}{2}\epsilon \alpha\beta km$ for all 
$M \in \sP$. This would imply that the objectives of 
\eqref{eq:optimization-noisy} and \eqref{eq:optimization-noiseless} are 
essentially identical, and so optimizing one also optimizes the other.

Using the inequality $|\langle \A - B, M \rangle| \leq \|\A-B\|_{\op}\|M\|_*$, 
where $\|\cdot\|_{\op}$ denotes operator norm, it suffices to establish a matrix 
concentration inequality showing that $\|\A - B\|_{\op}$ is small. 
\todo{Greg, can you fill this in with the relevant matrix concentration 
result?}

\paragraph{Part 2: bounding the influence of adversaries.} 
Next, we need to show that the nuclear norm constraint does 
not give the adversaries too much influence over the de-noised program 
\eqref{eq:optimization-noiseless}; this is the most novel aspect 
of our argument.

Suppose that the constraint on $\|M\|_*$ were not present in 
\eqref{eq:optimization-noiseless}. Then the adversaries would have 
no influence on $\Mm_{\good}$, because all the remaining constraints 
in \eqref{eq:optimization-noiseless} are separable across rows. 
How can we quantify the effect of this nuclear norm constraint?
We exploit Lagrangian duality, which allows us to replace constraints 
with appropriate modifications to the objective function.

\input lagrangian-figure

In this instance, 
Lagrangian duality guarantees that \eqref{eq:optimization-noiseless} has the 
same solution as a modified problem where the constraint on $\|M\|_*$ is removed, 
and the objective is modified to $\langle B - Z, M \rangle$, for an appropriately 
chosen $Z$. In particular, $Z$ lies in the subgradient of 
$\mu \|M\|_*$ for some $\mu > 0$.

To gain some intuition, consider 
Figure~\ref{fig:lagrangian}. In \eqref{eq:optimization-noiseless}, we optimize 
$\langle B, M \rangle$ subject to $M_{\good} = R_{\good}$, as well as subject to 
the nuclear norm and other constraints; this yields the optimal value $M^*$. 
We know by Lemma~\ref{lem:objective-bound} that $\langle B, \M \rangle$ is almost 
as large as $\langle B, M^* \rangle$, and would like to conclude that 
$\M_{\good} \approx M^*_{\good}$. Since $B$ points to the right, we know that 
decreasing $M_{\good}$ will decrease $\langle B, M \rangle$; the problem is that 
we may then be able to increase $M_{\bad}$, which would increase 
$\langle B, M \rangle$ again. The solution is to find a Lagrange 
multiplier $Z$ that supports 
the constraint on $\|M\|_*$, in the sense that optimizing 
$\langle B-Z, M \rangle$ yields $M^*$ even without the nuclear norm constraint. 
Then, $M_{\good}$ and $M_{\bad}$ become separable and we conclude that 
$\langle B_{\good} - Z_{\good}, M_{\good} \rangle$ must be within $\epsilon$ of 
$\langle B_{\good} - Z_{\good}, M^*_{\good} \rangle$, which implies that 
$M_{\good} \approx M^*_{\good}$.

If we formalize this intuition and analyze $Z$ in detail, we obtain the 
following result:
\begin{lemma}
\label{lem:subgradient}
Suppose that $k = \Omega\p{\frac{1}{\alpha\beta\epsilon^2}\frac{m}{n}}$. 
Then there exists a matrix $Z$ with 
$\rank(Z) = 1$, $\|Z\|_F \leq \epsilon \sqrt{\alpha\beta km/n}$ such that
\begin{align}
\langle \Aa_{\good} - Z_{\good}, \Mm_{\good} - M_{\good} \rangle &\leq \langle \Aa, \Mm - M \rangle \text{ for all $M \in \sP$}.
\end{align}
\end{lemma}
Essentially, Lemma~\ref{lem:subgradient} shows that any change in 
$\langle \Aa, M \rangle$ caused by changing $M_{\bad}$ can be upper-bounded 
by a small term $\langle Z_{\good}, \Mm_{\good} - M_{\good} \rangle$ that depends only 
on $M_{\good}$, thereby bounding the effect that the adversaries can have 
on $M_{\good}$. Lemma~\ref{lem:subgradient} is therefore the key 
technical tool powering our results, and is proved in 
Section~\ref{sec:subgradient-proof}.

\paragraph{Proof of Theorem~\ref{thm:main}.}
In the remainder of this section, we will prove Theorem~\ref{thm:main} from 
Lemmas~\ref{lem:objective-bound} and \ref{lem:subgradient}.
We start by plugging in $\M$ for $M$ in Lemma~\ref{lem:subgradient}. This yields
%\begin{align}
$\langle \Aa_{\good} - Z_{\good}, \Mm_{\good} - \M_{\good} \rangle \leq \langle \Aa, \Mm - \M \rangle \leq \epsilon \alpha\beta km$
%\end{align}
by Lemma~\ref{lem:objective-bound}.
On the other hand, we have 
\begin{align}
|\langle Z_{\good}, \Mm_{\good} - \M_{\good} \rangle| &\leq \|Z_{\good}\|_F\|\Mm_{\good} - \M_{\good}\|_F \\
 &\leq \epsilon\sqrt{\alpha\beta mk/n} \sqrt{\|\Mm_{\good} - \M_{\good}\|_{1}\|\Mm_{\good} - \M_{\good}\|_{\infty}} \\
 &\leq \epsilon\sqrt{\alpha\beta mk/n} \sqrt{2\alpha\beta mn} = \sqrt{2}\epsilon\alpha\beta km.
\end{align}
Putting these together, we obtain
%\begin{align}
$\langle \Aa_{\good}, \Mm_{\good} - \M_{\good} \rangle \leq (1+\sqrt{2})\epsilon \alpha\beta km$.
%\end{align}
Expanding $\langle \Aa_{\good}, \Mm_{\good} - \M_{\good} \rangle$ as 
$\frac{k}{n}\sum_{i \in \good}\p{\sum_{j \in [m]} (R_{ij} - \M_{ij})\Aavg_{ij}}$,
we obtain 
\[ \frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij}\Aavg_{ij}\Big) \leq (1+\sqrt{2})\epsilon. \]
Finally, by the $(L,\epsilon_0)$-Lipschitz condition, we have (see \ref{sec:lipschitz-justification} for details)
\begin{align}
\frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \ravg_j - \sum_{j \in [m]} \M_{ij}\ravg_j\Big) 
&\leq L \cdot \frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\Big(\sum_{j \in T^*} \Aavg_{ij} - \sum_{j \in [m]} \M_{ij}\Aavg_{ij}\Big) + \epsilon_0 \\
&\leq L \cdot (1+\sqrt{2})\epsilon + \epsilon_0.
\end{align}
The desired result then follows after scaling down $\epsilon$ 
by a factor of $1+\sqrt{2}$.

\subsection{Recovering $T$ (Algorithm~\ref{alg:recover-T})}
\label{sec:approach-T}
\label{sec:rounding}

The algorithm for recovering $Q_{\beta}(\rtrue)$ from $\M$ and $\robs$ 
is simple, and involves two steps:
\begin{enumerate}
\item Greedily choose the $\goodfrac n$ rows of $\M$ (call then $\goodobs$) 
      for which $\sum_{j \in [m]} \M_{ij}\robs_j$ is largest;
      let $\rdist \eqdef \frac{1}{\goodfrac n} \sum_{i \in \goodobs} \M_i$, and 
      $\sigma_j \eqdef \sum_{j'=1}^j \rdist_{j'}$.
\item Sample $u \sim \Uniform([0,1])$, and include in $T$ all $j$ for which 
      $\sigma_{j-1} \leq \frac{l + u}{\beta m} \leq \sigma_j$ 
      for some integer $l$. Resample $u$ until the resulting $T$ satisfies 
      \begin{equation}
      \label{eq:accept}
      \frac{1}{\beta m} \sum_{j \in T} \robs_j \leq \sum_{j \in [m]} \rdist_j \robs_j + 3\error.
      \end{equation}
\end{enumerate}
The validity of this procedure hinges on two results. First, we establish 
a concentration bound showing that $\sum_j \M_{ij}\robs_j$ is close to 
$\sum_j \M_{ij}\rtrue_j$, and hence the $\goodfrac n$ best rows of $\M$ 
according to $\robs$ also look good according to $\robs$:
\begin{proposition}
\label{prop:robs-rtrue}
Suppose that $\robs$ satisfies Assumption~\ref{ass:r} with 
$\fac_0 = \Omega\p{\frac{\log(1/\failprob\alpha)}{\alpha\beta\epsilon^2}}$. 
Then, with probability $1-\delta$, we have
\begin{equation}
\label{eq:robs-rtrue}
\frac{1}{\goodfrac n} \sum_{i \in \goodobs} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) \geq \frac{1}{|\good|} \sum_{i \in \good} \Big(\sum_{j \in [m]} \M_{ij}\rtrue_j\Big) - \epsilon.
\end{equation}
\end{proposition}
The idea is to establish a uniform bound showing that 
$\sum_{i \in S} \sum_{j \in [m]} \M_{ij}(\robs_j - \rtrue_j)$ is small for any 
set of $\goodfrac n$ rows $S$, and hence that greedily taking the $\goodfrac n$ 
best rows according to $\robs$ is almost as good as taking the $\goodfrac n$ 
best rows according to $\rtrue$. A na\"{i}ve union bound over all rows would 
yield $\log(n/\failprob)$ instead of $\log(1/\alpha\failprob)$ in the numerator 
for $\fac_0$; our proof includes a refined analysis that improves upon the union 
bound by leveraging the fact that we are averaging over a large number 
($\goodfrac n$) of rows and hence extreme events are less likely.

Proposition~\ref{prop:robs-rtrue} together with Theorem~\ref{thm:main-M} 
implies that $\sum_j \rdist_j \rtrue_j \geq \frac{1}{\beta m} \sum_{j \in Q_{\beta}(r^*)} \rtrue_j - 2\epsilon$. In a sense, we could stop here and be satisfied --- 
the distribution $\rdist$ is a good approximation to the uniform distribution 
over $Q_{\beta}(r^*)$, which is sufficient for many purposes. However, for 
completeness we would like to produce an actual set $T$ that approximates 
$Q_{\beta}(r^*)$. Proposition~\ref{prop:rounding} asserts that step $2$ above 
accomplishes this:

\begin{proposition}
\label{prop:rounding}
Suppose that the probability distribution $\rdist$ satisfies 
$\|\rdist\|_{\infty} \leq \frac{1}{\beta m}$. Then, for $T$ sampled 
as in step $2$ above, we have $|T| = \beta m$ and $\bE[\frac{1}{\beta m}\sum_{j \in T} \robs_j] = \sum_{j \in [m]} \rdist_j \robs_j$.
\end{proposition}
In particular, Proposition~\ref{prop:rounding} implies that, by 
Markov's inequality, \eqref{eq:accept} holds with probability at least 
$\frac{1}{3}$ for a given sample, and thus with probability 
$1-\delta$ we will accept within $3\log(1/\delta)$ steps.

\todo{this is a bit sloppy, clean this up}
\todo{unified theorem, with proof}

%%% TODO --- this is good intuition, should include it somewhere
%%% At a high level, our strategy is the following --- since $\Mm$ is already optimized 
%%% on the rows outside of $\good$, we should (``morally'', though not in actuality) have 
%%% $\langle \Aa_{\neg \good}, \Mm_{\neg \good} \rangle \geq \langle \Aa_{\neg \good}, \M_{\neg \good} \rangle$. 
%%% 
%%% Looking back at the rows inside $\good$, we must therefore have 
%%% $\langle \Aa_{\good}, \M_{\good} \rangle \geq \langle \Aa_{\good}, \Mm_{\good} \rangle - \epsilon \alpha n$, 
%%% which is exactly the statement \eqref{eq:M-bound} with $(1+\sqrt{2})\epsilon$ replaced by $\epsilon$.
%%% 
%%% The problem with the above argument is that, even though $\Mm$ is optimized on $\neg \good$, the rows in 
%%% $\good$ and $\neg \good$ are coupled by the joint constraint $\|M\|_* \leq \frac{2}{\epsilon\alpha}$. We 
%%% will decouple the rows by taking the subgradient of the Lagrangian at $\Mm$, which yields an upper bound 
%%% for our optimization problem. By doing this we obtain:

