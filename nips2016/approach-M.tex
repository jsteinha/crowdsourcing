\section{Recovering $\M$ (Algorithm~\ref{alg:recover-M})}
\label{sec:approach-M}

The goal of this section is to show that solving the optimization 
problem \eqref{eq:optimization-noisy} recovers a matrix $\M$ that 
approximates the $\beta$-quantile of $\ravg$ in the following sense:
\begin{proposition}
\label{prop:recover-M}
Under the conditions of Theorem~\ref{thm:main}, Algorithm~\ref{alg:recover-M} 
outputs a matrix $\M$ satisfying 
\[ \frac{1}{|\good|} \frac{1}{\beta m} \sum_{i \in \good} \sum_{j \in [m]} (T^*_j - \M_{i,j})\Aavg_{ij} \leq \epsilon, \]
where $T^*_j = 1$ if $j$ lies in the $\beta$-quantile of $\ravg$, and is $0$ otherwise.
\end{proposition}
Proposition~\ref{prop:recover-M} says that the row $\M_i$ 
is good according to rater $i$'s ratings $\Aavg_i$. Note that
$(L,\epsilon_0)$-monotonicity 
then implies that $\M_i$ is also good according to $\ravg$.
In particular (see \ref{sec:lipschitz-details} for details)
\begin{align}
\label{eq:lipschitz-usage}
\frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\sum_{j \in [m]} (T_j^*-\M_{ij})\ravg_j
&\leq L \cdot \frac{1}{|\good|}\frac{1}{\beta m}\sum_{i \in \good}\sum_{j \in [m]} (T_j^*-\M_{ij})\Aavg_{ij} + \epsilon_0 \\
&\leq L \cdot \epsilon + \epsilon_0.
\end{align}

Proving Proposition~\ref{prop:recover-M} involves two major steps: showing 
(a) that the nuclear norm constraint in \eqref{eq:optimization-noisy} 
imparts noise-robustness, and (b) that the constraint does not allow 
the adversaries to influence $\M_{\good}$ too much. (For a matrix $X$ 
we let $X_{\good}$ denote the rows indexed by $\sC$ and $X_{\bad}$ the remaining rows.)

In a bit 
more detail, if we let $\Mm$ denote a denoised version of $\M$, and $\Aa$ 
denote a denoised version of $\A$, we will first show 
(Lemma~\ref{lem:objective-bound}) that 
$\langle B, \M - \Mm \rangle \geq -\epsilon'$ for some $\epsilon'$ 
determined below. This can be established via the matrix concentration 
inequalities in \citet{le2015concentration}. Lemma~\ref{lem:objective-bound} already 
suffices for standard approaches \citep[e.g.,][]{guedon2014community}, 
but in our case we must grapple with the issue that the rows of $B$ could be 
arbitrary outside of $\sC$, and hence closeness according to $B$ may not 
imply actual closeness between $\M$ and $\Mm$. This leads to our main 
novel technical contribution, Lemma~\ref{lem:subgradient}, where we show 
that $\langle B_{\good}, \M_{\good} - \Mm_{\good} \rangle \geq \langle B, \M - \Mm \rangle - \epsilon'$; 
that is, \emph{closeness according to $B$ implies closeness according to 
$B_{\good}$}. We can then restrict the remainder of our analysis to the 
reliable raters, and obtain Proposition~\ref{prop:recover-M}.

\paragraph{Part 1: noise-robustness.} Let $\Aa$ be the matrix satisfying 
$\Aa_{\good} = \frac{k}{m}\Aavg_{\good}$, $\Aa_{\bad} = \Aobs_{\bad}$, 
which denoises $\A$ on $\good$.
%Here we 
%let $A_{\good}$ denote the subset of rows of $A$ indexed by $\good$, and 
%$A_{\bad}$ the remaining rows. 
The scaling $\frac{k}{m}$ is chosen so that 
$\bE[\Aobs_{\good}] \approx \Aa_{\good}$.
Also define $R \in \bR^{n \times m}$ by $R_{ij} = T_j^*$.
%where $\|\cdot\|_*$ denotes the nuclear norm. In the sequel, 
%we use $\sP$ to denote the constraint set in (\ref{eq:optimization-noisy}).

Ideally, we would like to have $M_{\good} = R_{\good}$, i.e., $M$ matches $T^*$ on 
all the rows of $\good$. In light of this, 
we will let $\Mm$ be the solution to the following ``corrected'' program, which 
we don't have access to (since it involves knowledge of $\Aavg$ and $\good$), 
but which will be useful for analysis purposes:
%\begin{align}
%\label{eq:optimization-noiseless}
%\text{maximize } &\langle \Aa, M \rangle, \\
%\notag \text{ subject to } &M_{\good} = R_{\good}, \\
%\notag  &M_{ij} \geq 0 \,\,\, \forall i,j, \\
%\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
%\notag  &\max_j M_{ij} \leq \frac{1}{\beta m}\sum_j M_{ij} \,\,\, \forall i, \\
%\notag  &\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}.
%\end{align}
\begin{align}
\label{eq:optimization-noiseless}
\text{maximize } &\langle \Aa, M \rangle, \\
\notag \text{ subject to } &M_{\good} = R_{\good}, 
  &&\hskip-0.4in 0 \leq M_{ij} \leq 1 \,\,\, \forall i,j,  \\
%  &&\hskip-0.4in\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &{\textstyle \sum_j} M_{ij} \leq \beta m \,\,\, \forall i, 
%  &&\hskip-0.4in\|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\frac{\alpha n}{\beta m}}, \phantom{xxxxxxx}
  &&\hskip-0.4in \|M\|_* \leq \frac{2}{\alpha\epsilon}\sqrt{\alpha\beta nm} \phantom{xxxxxxx}
\end{align}
Importantly, \eqref{eq:optimization-noiseless} enforces $\Mm_{ij} = T_j^*$ for all 
$i \in \good$. Our goal is to show that $\M$ is ``close'' to $\Mm$. 
The following lemma formalizes this:
%Following an idea of \citet{guedon2014community}, we have the following result:
\begin{lemma}
\label{lem:objective-bound}
Let $m \geq n$. Suppose that Assumption~\ref{ass:independent} holds and that 
$k = \Omega\p{ \frac{\log^3(1/\delta)}{\beta\alpha^3\eps^4}\frac{m}{n}}$. 
Then, the solution $\M$ to \eqref{eq:optimization-noisy} performs nearly as 
well as $\Mm$ under $B$; specifically,
\begin{equation}
\label{eq:objective-bound}
\langle \Aa, \M \rangle \geq \langle \Aa, \Mm \rangle - \epsilon \alpha\beta kn.
\end{equation}
\end{lemma}
Note that $\M$ is not necessarily feasible for \eqref{eq:optimization-noiseless}, 
because of the constraint $M_{\good} = R_{\good}$; Lemma~\ref{lem:objective-bound} 
merely asserts that $\M$ approximates $\Mm$ in objective value. The proof of 
Lemma~\ref{lem:objective-bound}, given in Section~\ref{sec:objective-bound-proof}, 
primarily involves establishing a 
\emph{uniform deviation result}; if we let $\sP$ denote the feasible set for 
\eqref{eq:optimization-noisy}, then we wish to show that 
$|\langle \A - B, M \rangle| \leq \frac{1}{2}\epsilon \alpha\beta kn$ for all 
$M \in \sP$. This would imply that the objectives of 
\eqref{eq:optimization-noisy} and \eqref{eq:optimization-noiseless} are 
essentially identical, and so optimizing one also optimizes the other.

Using the inequality $|\langle \A - B, M \rangle| \leq \|\A-B\|_{\op}\|M\|_*$, 
where $\|\cdot\|_{\op}$ denotes operator norm, it suffices to establish a matrix 
concentration inequality bounding $\|\A - B\|_{\op}$.   
This bound follows from the general matrix concentration result of~\cite{le2015concentration},
stated in Section~\ref{sec:le-statement}.

\paragraph{Part 2: bounding the influence of adversaries.} 
We next show that the nuclear norm constraint does 
not give the adversaries too much influence over the de-noised program 
\eqref{eq:optimization-noiseless}; this is the most novel aspect 
of our argument.

Suppose that the constraint on $\|M\|_*$ were not present in 
\eqref{eq:optimization-noiseless}. Then the adversaries would have 
no influence on $\Mm_{\good}$, because all the remaining constraints 
in \eqref{eq:optimization-noiseless} are separable across rows. 
How can we quantify the effect of this nuclear norm constraint?
We exploit Lagrangian duality, which allows us to replace constraints 
with appropriate modifications to the objective function.

\input lagrangian-figure

In this instance, 
Lagrangian duality guarantees that \eqref{eq:optimization-noiseless} has the 
same solution as a modified problem where the constraint on $\|M\|_*$ is removed, 
and where the objective is modified to $\langle B - Z, M \rangle$, for an 
appropriately chosen $Z$. In particular, $Z$ lies in the subgradient of 
$\mu \|M\|_*$ for some $\mu > 0$.

To gain some intuition, consider 
Figure~\ref{fig:lagrangian}. In \eqref{eq:optimization-noiseless}, we optimize 
$\langle B, M \rangle$ subject to $M_{\good} = R_{\good}$, as well as subject to 
the nuclear norm and other constraints; this yields the optimal value $M^*$. 
We know by Lemma~\ref{lem:objective-bound} that $\langle B, \M \rangle$ is almost 
as large as $\langle B, M^* \rangle$, and would like to conclude that 
$\M_{\good} \approx M^*_{\good}$. Since $B$ points to the right in Figure~\ref{fig:lagrangian}, 
decreasing $M_{\good}$ will decrease $\langle B, M \rangle$. The problem is that 
we may then be able to increase $M_{\bad}$, which would offset 
this decrease; then $M_{\good}$ could potentially decrease quite a 
bit with minimal effect on $\langle B, M \rangle$. 
To rule this out, we find a Lagrange 
multiplier $Z$ that supports 
the constraint $\|M\|_* \leq \rho$, in the sense that optimizing 
$\langle B-Z, M \rangle$ yields $M^*$ even without the constraint. 
Then, $M_{\good}$ and $M_{\bad}$ become separable and we conclude that 
$\langle B_{\good} - Z_{\good}, \M_{\good} \rangle$ must be within $\epsilon$ of 
$\langle B_{\good} - Z_{\good}, M^*_{\good} \rangle$, which implies that 
$\M_{\good} \approx M^*_{\good}$.

If we formalize this intuition and analyze $Z$ in detail, we obtain the 
following result:
\begin{lemma}
\label{lem:subgradient}
Let $m \geq n$. Suppose that $k = \Omega\p{\frac{\log^3(1/\delta)}{\alpha\beta\epsilon^2}\frac{m}{n}}$. 
Then with probability at least $1-\delta$ there exists a matrix $Z$ with 
$\rank(Z) = 1$, $\|Z\|_F \leq \epsilon k\sqrt{\alpha\beta n/m}$ such that
\begin{align}
\langle \Aa_{\good} - Z_{\good}, \Mm_{\good} - M_{\good} \rangle &\leq \langle \Aa, \Mm - M \rangle \text{ for all $M \in \sP$}.
\end{align}
\end{lemma}
Essentially, Lemma~\ref{lem:subgradient} shows that any change in 
$\langle \Aa, M \rangle$ caused by changing $M_{\bad}$ can be upper-bounded 
by a small term $\langle Z_{\good}, \Mm_{\good} - M_{\good} \rangle$ that depends only 
on $M_{\good}$, thereby bounding the effect that the adversaries can have 
on $M_{\good}$. Lemma~\ref{lem:subgradient} is therefore the key 
technical tool powering our results, and is proved in 
Section~\ref{sec:subgradient-proof}. Proposition~\ref{prop:recover-M} 
is proved from Lemmas~\ref{lem:objective-bound} and \ref{lem:subgradient} 
in Section~\ref{sec:recover-M-proof}.

