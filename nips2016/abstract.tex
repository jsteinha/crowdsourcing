We consider a crowdsourcing model in which $n$ workers are asked to 
rate the quality of $n$ items previously generated by other workers.
An unknown set of $\goodraters n$ workers generate reliable ratings, 
while the remaining workers may behave arbitrarily and possibly adversarially. 
The manager of the experiment can also manually evaluate the quality of a 
small number of items, and wishes to curate together almost all 
of the high-quality items with at most an $\error$ fraction of 
low-quality items.  
Perhaps surprisingly, we show that this is possible with an 
amount of work required of the manager, and each worker, that does not scale 
with $n$: the dataset can be curated with
$\tilde{\oo}\p{\frac{1}{\beta\goodraters^3\error^4}}$ 
ratings per worker, and $\tilde{\oo}\p{\frac{1}{\beta\error^2}}$ 
ratings by the manager, where $\beta$ is the fraction of batches that are 
high-quality.
Our results extend to the more general setting of peer prediction, 
including peer grading in online classrooms.
