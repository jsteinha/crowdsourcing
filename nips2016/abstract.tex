We consider a crowdsourcing model in which $n$ workers are each asked to 
rate the quality of data previously generated by other workers.
A fraction $\goodraters$ of workers generate reliable ratings, 
while the remaining workers may behave arbitrarily and possibly adversarially. 
The manager of the experiment can also manually evaluate the quality of a 
small amount of data, and wishes to curate a dataset containing almost all 
of the high-quality data with at most an $\error$ fraction of low-quality data.  
Perhaps surprisingly, we show that this is possible with an 
amount of work required of the manager, and each worker, that does not scale 
with $N$: the dataset can be curated with
$\tilde{\oo}\p{\frac{1}{\beta\goodraters^3\error^4}}$ 
work per worker, and $\tilde{\oo}\p{\frac{1}{\beta\error^2}}$ 
work for the manager, where $\beta$ is the fraction of data that is high-quality.
Our results extend to the more general setting of peer prediction, 
including peer grading in online classrooms.
