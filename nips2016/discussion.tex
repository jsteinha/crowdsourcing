\section{Open Directions and Related Work}
\label{sec:discussion}
\iffalse
\todo{fill in with lower bounds and conjectures that explain what 
``best possible'' result would look like, and how adversarial and 
stochastic settings compare}\fi

\paragraph{Future Directions.}
On the theoretical side, perhaps the most immediate open question is whether it is 
possible to improve the dependence of $k$ (the amount of work required per worker) 
on the parameters $\alpha, \beta,$ and $\eps$.  It is tempting to hope that 
when $m = n$ a tight result would have 
$k = \tilde{\oo}\p{\tfrac{1}{\alpha\beta\eps^2}}$, in loose analogy
to recent results for the stochastic block model \citepm{abbe2015detection,abbe2015community,banks2016information}. 
For stochastic block models, there is conjectured to be a gap between 
computational and information-theoretic thresholds, and it would be 
interesting to see if a similar phenomenon holds here (the scaling for 
$k$ given above is based on the conjectured computational threshold).
%Interestingly, there is likely to be a computational gap ,banks2016information}.
%\todo{is this the right answer here / citations? double-check}

A second open question concerns the scaling in $n$:
if $n \gg m$, can we get by with much less work per rater?
Finally, it would be interesting to consider adaptivity: if 
the choice of queries is based on previous worker ratings, 
can we reduce the amount of work?


\paragraph{Related work.}
Our setting is closely related to the problem of \emph{peer prediction} 
\citepm{miller2005eliciting}, in which we wish to obtain truthful information 
from a population of raters by exploiting inter-rater agreement. 
While several mechanisms have been proposed for these tasks, 
they typically assume that rater accuracy is observable online
\citepm{resnick2007influence}, that the dishonest raters are 
rational agents maximizing a payoff function \citepm{dasgupta2013crowdsourced,
kamble2015truth,shnayder2016strong}, that the raters follow a simple 
statistical model \citepm{karger2014budget,zhang2014crowdsourcing,
zhou2015regularized}, or some combination of the above \citepm{shah2015double,
shah2015approval}. 
\citetm{ghosh2011moderates} allow $o(n)$ adversaries 
to behave arbitrarily but require the rest to be stochastic.

%In crowdsourcing, it is common to use ``gold sets'' -- questions where the 
%ground truth is known -- to assess worker performance. 
%However, there are tasks for which gold sets do not make sense
%--- for instance, ``draw an interesting picture'' or ``translate this sentence'', 
%where there are different equally good answers. In this case, the only way to 
%evaluate quality is via evaluation by other workers or by the manager of 
%the experiment. Even for more straightforward tasks, \citetm{vuurens2011spam} 
%suggest that workers may be able to identify gold set questions in some 
%instances. Beyond crowdsourcing, there are settings where human judgment is 
%necessary and where there are documented attempts to game the system --- 
%\citetm{harmon2004amazon} reports that many authors and filmmakers 
%engage in dishonest tactics to inflate their reviews on amazon.com, while 
%\citetm{priedhorsky2007creating} find that roughly $5\%$ of revisions on 
%wikipedia are damaged in some way (e.g. by vandalism). In such settings where 
%content is primarily moderated by the crowd, building resilient ratings 
%mechanisms is important for maintaining quality.

The work closest to ours is \citetm{christiano2014provably,
christiano2016robust}, which studies online collaborative prediction in 
the presence of adversaries; roughly, when raters interact with an item 
they predict its quality and afterwards observe the actual quality; the 
goal is to minimize the number of incorrect 
predictions among the honest raters. This differs from our setting in that 
(i) the raters are trying to learn the item qualities as part of the task, 
and (ii) there is no requirement to induce a final global estimate of the 
high-quality items, which is necessary for estimating quantiles.
It seems possible however that there are theoretical ties between this 
setting and ours, which would be interesting to explore.

{\small
\paragraph{Acknowledgments.}
JS was supported by a Fannie \& John Hertz Foundation Fellowship,
an NSF Graduate Research Fellowship, and a Future of Life Institute 
grant. GV was supported by NSF CAREER 
award CCF-1351108, a Sloan Foundation Research Fellowship, and a research 
grant from the Okawa Foundation. MC was supported by NSF grants 
CCF-1565581, CCF-1617577, CCF-1302518 and a Simons Investigator Award.}
