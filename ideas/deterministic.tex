\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{hyperref,url}
\usepackage{import,subfiles}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{fullpage}
\input{latex-defs.tex}
\usepackage{breqn}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\M}{\hat{M}}

\begin{document}

\section{Deterministic Case}
Suppose we have a symmetric matrix $A \in \bR^{n \times n}$ satisfying 
\begin{itemize}
\item $A_{i,j} = A_{i',j}$ for all $i,i' \in \sC$
\item $A_{i,i'} = 1$ for all $i,i' \in \sC$
\end{itemize}
where $\sC \subset \{1,\ldots,n\}$ has size $\alpha N$.

Let $\M$ be the solution to the following program:
\begin{equation}
\label{eq:optimization}
\text{maximize } \Tr(A^{\top}M) - \frac{\alpha N}{4} \|M\|_*, \text{ subject to } 0 \leq M_{ij} \leq 1 \, \forall i,j,
\end{equation}
where $\|\cdot\|_*$ denotes the nuclear norm. Also let $w_i = 1$ if $i \in \sC$ and $0$ otherwise.

We prove the following two structural lemmas, which show that 
$\M$ (approximately) inherits the properties of $A$:
\begin{lemma}
\label{lem:constant}
We have $\M_{i,j} = \M_{i',j}$ for all $i,i' \in \sC$.
\end{lemma}

\begin{lemma}
\label{lem:ones}
There is a set $\sC_0 \subseteq \sC$ of size at least 
$\frac{\alpha N}{4}$ such that 
$\M_{i,i'} = 1$ for all $i \in \sC$, $i' \in \sC_0$.
\end{lemma}
The advantage of using $\M$ instead of $A$ is that \eqref{eq:optimization} is highly 
noise-robust. Moreover, given $\M$, we can efficiently identify $\sC$: first sample 
a vertex at random, and with probability $\frac{\alpha}{4}$ obtain some $v \in \sC_0$. 
Then, take the set $\sC_1$ of all $j$ such that $M_{v,j} = 1$. Finally, take the set 
$\tilde{\sC}$ of all $i \in \sC_1$ such that $M_{i,j} = 1$ for all $j \in \sC_1$. 
We can show that the set $\tilde{\sC}$ is a clique containing $\sC_0$:
\begin{lemma}
\label{lem:clique}
The set $\tilde{\sC}$ is a clique ($M_{i,i'} = 1$ for all $i,i' \in \tilde{\sC}$). 
Moreover, $\sC_0 \subseteq \tilde{\sC}$.
\end{lemma}
\begin{proof}
First, $\tilde{\sC}$ satisfies $M_{i,j} = 1$ for all $i \in \tilde{\sC}, j \in \sC_1$, 
and $\tilde{\sC} \subseteq \sC_1$, so in particular $M_{i,j} = 1$ for all $i,j \in \tilde{\sC}$ 
and hence $\tilde{\sC}$ is a clique.

Second, $\sC_0 \subseteq \sC_1$, and furthermore any $i \in \sC_0$ satisfies 
$M_{i,j} = M_{v,j} = 1$ for all $j \in \sC_1$, so $\sC_0 \subseteq \tilde{\sC}$.
\end{proof}

We now prove the two structural lemmas.
\begin{proof}[Proof of Lemma~\ref{lem:constant}]
Consider the matrix $P \in \bR^{n \times n}$ defined by 
$P_{i,i'} = \frac{1}{|\sC|}$ if $i,i' \in \sC$, and 
$P_{i,i'} = \delta_{i,i'}$ otherwise. Then $P$ is clearly a 
projection matrix, and hence $\|PM\|_* \leq \|M\|_*$ for all 
matrices $M$. Furthermore, $\Tr(A^{\top}(PM)) = \Tr(A^{\top}M)$. 
It follows that replacing $M$ by $PM$ weakly increases the value of 
\eqref{eq:optimization}, and hence we must have $M = PM$, from which 
the result follows.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:ones}]
Let $\M_{\sC}$ be the submatrix of $\M$ restricted 
to the rows and columns of $\sC$. By Lemma~\ref{lem:constant}, 
$\M_{\sC}$ takes the form $\bI c^{\top}$ for some vector 
$c \in [0,1]^{\sC}$. Now, suppose for the sake of contradiction 
that less than $\frac{\alpha N}{4}$ of the entries of $c$ 
are equal to $1$. Then there is a vector 
$s \in \{0,1\}^{\sC}$, containing $\frac{\alpha N}{4}$ 
$1$'s, such that it is feasible to move $c$ in the direction of 
$s$, such that $M_{\sC} = \bI (c+\epsilon s)^{\top}$. This 
increases $\Tr(A^{\top}M)$ by $\frac{1}{4}\epsilon \alpha^2 N^2$, 
and increases $\frac{\alpha N}{4}\|M\|_*$ by at most 
$\frac{\epsilon \alpha N}{4}\|\bI s^{\top}\|_* = \frac{1}{8}\epsilon \alpha^2 N^2$. 
Therefore, moving in the direction of $s$ increases the value of 
\eqref{eq:optimization}, which contradicts the fact that $\M$ is optimal.
\end{proof}

\section{Noise-Robustness}

We next show that the optimization problem \eqref{eq:optimization} is 
noise-robust. More formally, we will replace $A$ with a new matrix $B$ 
such that $\bE[B] = A$; our goal will be to show that this does 
not change the optimum of \eqref{eq:optimization} by very much.

Our particular choice of $B$ will be a sub-sampling of $A$. Formally, 
we will take $B_{i,j} = X_{i,j}A_{i,j}$, where $X_{i,j} = \frac{1}{p}$ 
with probability $p$ and $0$ otherwise.

\begin{proposition}
Suppose that $B$ is constructed as above, and let $f(M) = \Tr(B^{\top}M) - \mu \|M\|_*$. 
Then, if $\mu \geq \frac{2}{\sqrt{p}}\p{5\sqrt{N} + \sqrt{\log\p{\frac{1}{\delta}}}}$, we 
have $f(PM) \geq f(M)$ for all $M$ with probability $1-\delta$.
\end{proposition}

\begin{proof}
Fix $M$, and let $PM = U\Sigma V^{\top}$ be the singular value 
decomposition of $PM$. We note that 
$f(M) \leq f(PM) + \langle B + \mu Z, (I-P)M \rangle$ for 
any matrix $Z$ satisfying $\|Z\|_{\op} \leq 1$ and $\langle Z, PM \rangle = \|PM\|_*$.
We make the following claim:
\begin{lemma}
For any $M$, we have
\[ \inf\{ \langle Z, (I-P)M \rangle \mid \|Z\|_{\op} \leq 1,\; \langle Z, PM \rangle = \|PM\|_*\} = -\|(I-P)M\|_*. \]
\end{lemma}
We also note that $\langle A, (I-P)M \rangle = 0$. Putting these together, we have
\begin{align}
f(M) &\leq f(PM) + \langle B-A, (I-P)M \rangle - \mu \|(I-P)M\|_* \\
 &\leq f(PM) + \p{\|(B-A)_{\sC}\|_{\op} - \mu} \|(I-P)M\|_*,
\end{align}
where $(B-A)_{\sC}$ denotes the rows of $B-A$ indexed by $\sC$.
Thus, as long as $\|(B-A)_{\sC}\|_{\op} \leq \mu$, we will have $f(M) \leq f(PM)$ for all $M$.
For simplicity of notation let $Y \eqdef (B-A)_{\sC}$ Note that $Y$ is an $(\alpha N) \times N$ 
matrix, each row of which is an independent zero-mean random variable with variance at most $\frac{1}{p}-1$. 
We make use of the following matrix concentration inequality (basically Corollary 3.9 of \url{http://arxiv.org/abs/1408.6185}):
\begin{theorem}
$\bP\left[\|(B-A)_{\sC}\|_{\op} \geq \frac{2}{\sqrt{p}}\p{5\sqrt{n} + t}\right] \leq \exp\p{-t^2}$.
\end{theorem}
This yields the desired result.
\end{proof}
Since we would like to set $\mu \approx \frac{\alpha N}{4}$, this yields $p = \Theta\p{\frac{1}{\alpha^2N}}$.

\end{document}
