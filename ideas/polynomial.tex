\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{hyperref,url}
\usepackage{import,subfiles}
\usepackage{tabularx}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{fullpage}
\usepackage{natbib}
\input{latex-defs.tex}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
  pdftitle={},
  pdfauthor={},
  pdfsubject={Proceedings of the International Conference on Machine Learning 2016},
  pdfkeywords={},
  pdfborder=0 0 0,
  pdfpagemode=UseNone,
  colorlinks=true,
  linkcolor=mydarkblue,
  citecolor=mydarkblue,
  filecolor=mydarkblue,
  urlcolor=mydarkblue,
  pdfview=FitH}
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\usepackage{breqn}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\M}{\tilde{M}}
\newcommand{\A}{\hat{A}}
\DeclareMathOperator{\poly}{poly}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\oo}{\mathcal{O}}
\newcommand{\bi}{\mathbbm{1}}
\newcommand{\todo}[1]{{\color{red} [TODO: {#1}]}}
\DeclareMathOperator{\round}{round}

\begin{document}

\section{Achieving $\poly(\alpha^{-1})$ degree}
We assume we have a matrix $A \in \bR^{n \times n}$ and sets 
$\sC \subset [n], \sD \subset [n]$ satisfying the following:
\begin{itemize}
\item $A_{i,j} = +1$ for all $i \in \sC$, $j \in \sD$
\item $A_{i,j} = -1$ for all $i \in \sC$, $j \not\in \sD$
\end{itemize}
Moreover, assume that $\min(|\sC|, |\sD|) \geq \alpha n$.

Finally, we let $\A$ be a random matrix with i.i.d. entries such that $\bE[\A] = A$, 
and $\bE[(\A_{ij}-A_{ij})^{2p}] \leq \frac{(2p)!}{2^pp!} \p{n/d}^{p}$ for all $p \in \{1,2,\ldots\}$ 
and some fixed $d \in \bR_{>0}$. For instance, this holds if $\Var[\A_{ij}] \leq \frac{n}{d}$ and $|\A_{ij}| \leq \frac{n}{d}$ almost surely, 
which we can obtain by sampling each entry of $A$ with probability $\frac{d}{n}$ and scaling appropriately (which results in $d$ entries in 
each row in expectation).
\todo{the moment assumption doesn't actually hold in this case, and making it hold costs us a $\sqrt{\log(n)}$ factor}

Let $\M$ be the solution to the following program:
\begin{align}
\label{eq:optimization-noisy}
\text{maximize } &\langle \A, M \rangle, \\
\notag \text{ subject to } &M_{ij} \geq 0 \,\,\, \forall i,j, \\
\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\|M\|_* \leq \frac{1}{\alpha},
\end{align}
where $\|\cdot\|_*$ denotes the nuclear norm.

We also let $M^*$ be the solution to the ``noiseless'' program
\begin{align}
\label{eq:optimization-noiseless}
\text{maximize } &\langle A, M \rangle, \\
\notag \text{ subject to } &M_{ij} \geq 0 \,\,\, \forall i,j, \\
\notag  &\sum_j M_{ij} \leq 1 \,\,\, \forall i, \\
\notag  &\|M\|_* \leq \frac{1}{\alpha},
\end{align}

Our goal is to show that $\M$ is ``close'' to giving correct information about $\sC$ and $\sD$. 
We will proceed in $3$ steps:
\begin{enumerate}
\item Show that $\M$ is an approximate optimizer of \eqref{eq:optimization-noiseless}.
\item Characterize the optimum $M^*$ of \eqref{eq:optimization-noiseless}, and show that it identifies $\sC$ and $\sD$.
\item Show that any approximate optimizer of \eqref{eq:optimization-noiseless} is close to $M^*$.
\end{enumerate}
All relevant lemmas are proved in Section~\ref{sec:lemma-proofs}.

\section{Analyzing $\M$}
For the first step, let $\sP$ denote the constraint set in (\ref{eq:optimization-noisy},\ref{eq:optimization-noiseless}). 
We start with the following inequality:
\begin{lemma}
\label{lem:concentration}
With probability $1-\delta$, we have
\[ |\langle A-\A, M \rangle| \leq \oo\p{\frac{n}{\alpha}\p{\frac{1}{\sqrt{d}} + \frac{1}{d}\sqrt{\log(1/\delta)}}} \]
for all $M \in \sP$.
\end{lemma}
With this lemma, we can show that $\M$ approximately optimizes \eqref{eq:optimization-noiseless}. 
We will take $d = \Omega\p{\max\p{\frac{1}{(\alpha^2\epsilon)^2}, \frac{\sqrt{\log(1/\delta)}}{\alpha^2\epsilon}}}$, which yields (for appropriate constants)
\begin{align}
|\langle A-\A, M \rangle| &\leq \frac{1}{2}\epsilon \alpha n.
\end{align}
We then have
\begin{align}
\langle A, \M \rangle  &\geq \langle \A, \M \rangle - \frac{1}{2}\epsilon \alpha n \\
 &\geq \langle \A, M^* \rangle - \frac{1}{2} \epsilon \alpha n \\
 &\geq \langle A, M^* \rangle - \epsilon \alpha n.
\end{align}
It follows that $\M$ is within $\epsilon \alpha n$ of the optimum of \eqref{eq:optimization-noiseless}, 
at least in terms of objective value.

\section{Analyzing $M^*$}
We next show that the optimal $M^*$ identifies $\sC$ and $\sD$ in the following sense:
\begin{proposition}
For all $i \in \sC$, we have $M^*_{ij} = \frac{1}{|\sD|}\bI[j \in \sD]$.
\end{proposition}
\begin{proof}
Let $R$ be the matrix such that $R_{ij} = \frac{1}{|\sD|}\bI[i \in \sC, j \in \sD]$. Note 
that $\|R\|_* = \sqrt{\frac{|\sC|}{|\sD|}}$ while $\langle A, R \rangle = |\sC|$. 
\end{proof}

\section{Bounding $M^* - \M$}
To start, we need some notation. Let $r(M)$ be the matrix such that
\[ r(M)_{ij} = \left\{ \begin{array}{ccl} \frac{1+A_{ij}}{2} & : & i \in \sC \\ M_{ij} & : & \text{else} \end{array} \right.. \]
In other words, $r$ replaces the rows in $\sC$ with their target values, and 
leaves the rest of the rows fixed.

We also define the projection matrix $P$ as
\[ P_{i,i'} = \left\{ \begin{array}{ccl} \frac{1}{|\sC|} & : & i, i' \in \sC \\ \delta_{i,i'} & : \text{else} \end{array} \right.. \]
Thus $PM = M$ if and only if all rows in $\sC$ are equal to each other.

We have the following technical lemma which controls 
the subgradient of the objective function:
\begin{lemma}
\label{lem:subgradient}
Let $f(M) = \langle A, M \rangle - \mu \|M\|_*$, and let 
$M_0$ satisfy $M_0 = PM_0$. Then,
there is a $Z_0 \in \partial f(M_0)$ such that, when restricted 
to the rows lying in $\sC$, we have
\[ (Z_0)_{\sC} = \bi \cdot \left[(2\bi_{\sD}-\bi)^{\top} - \mu v_0^{\top}\right], \]
for some vector $v_0$ of norm at most $\frac{1}{\sqrt{|\sC|}}$.
\end{lemma}

We are now ready to show that $\M$ is close to $M^*$. Indeed, we already 
have that $f(\M) \geq f(M^*) - \epsilon \alpha^2 N^2$. Letting $M' \eqdef r(\M)$ 
and re-arranging, we have
\begin{align}
\epsilon \alpha^2 N^2 &\geq f(M^*) - f(\M) \\
 &\geq f(M') - f(\M) \\
 &\geq \langle M' - \M, Z \rangle,
\end{align}
where $Z \in \partial f(M')$.
By Lemma~\ref{lem:subgradient}, we know that $Z_{\sC} = \bi \cdot \left[(2\bi_{\sD}-\bi)^{\top} - \mu v^{\top}\right]$, 
where $\|v\|_2^2 \leq \frac{1}{|\sC|}$. Since $M'_{ij} = 1$ if $j \in \sD$ and $0$ if $j \not\in D$, and 
$\M_{ij} \in [0,1]$, we have $Z_{ij}(M'_{ij}-\M_{ij}) = |M'_{ij} - \M_{ij}| - \mu v_j(\M'_{ij}-\M_{ij})$, and hence
\begin{align}
\langle M' - \M, Z \rangle &= \sum_{i \in \sC, j} |M'_{ij} - \M_{ij}| - \mu v_j (M'_{ij} - \M_{ij}) \\
 &\geq \|M'-\M|_1 - \mu \sqrt{|\sC|\sum_j v_j^2}\sqrt{\sum_{i \in \sC, j} (M_{ij}'-\M_{ij})^2} \\
 &\geq \|M'-\M\|_1 - \mu \|M' - \M\|_F.
\end{align}
In all, we see that $\|M'-\M\|_1 \leq \epsilon \alpha^2 N^2 + \mu \|M' - \M\|_F \leq 2\epsilon \alpha^2 N^2$.
Since $M' = r(\M)$, this implies the final inequality:
\begin{theorem}
\label{thm:bound}
If $\M$ is the solution to \eqref{eq:optimization-noisy} with $\mu$ and $d$ as defined 
above, then
\[ \sum_{i \in \sC, j} \left|\M_{ij} - \frac{1+A_{ij}}{2}\right| \leq 2\epsilon\alpha^2N^2. \]
In particular, we have
\[ \#\{i \in \sC, j \in [n] \mid \sign(2M_{ij}-1) \neq A_{ij}\} \leq 4\epsilon\alpha^2N^2. \]
\end{theorem}
Let $\round(x) = 1$ if $x \geq \frac{1}{2}$ and $0$ if $x < \frac{1}{2}$. An immediate consequence 
of Theorem~\ref{thm:bound} is that 
there are at least $\frac{1}{2}\alpha N$ rows $i$ for which $\round(A_{ij}) = \bI[j \in \sD]$ 
for all but $8\epsilon \alpha N$ values of $j$. Therefore, if we sample a row randomly, with 
reasonable (i.e., $\frac{\alpha}{2}$) probability that row will tell us all but an $\oo(\epsilon)$ fraction of $\sD$.

\section{Proofs of Lemmas}
\label{sec:lemma-proofs}

\begin{proof}[Proof of Lemma~\ref{lem:concentration}]
We have that 
\[ \sup_{\|M\|_* \leq n^2/\mu} |\langle A-\A, M \rangle| = \frac{n^2}{\mu}\|A-\A\|_{\op}. \]
Therefore, our task is to bound $\|A-\A\|_{\op}$. For this, we make use of \citet{bandeira2014sharp}, 
Theorem 3.1 \& Corollary 3.2, which states that, for any $\epsilon > 0$, we have that
\begin{align}
\bE[\|A-\A\|_{\op}] &\leq (1+\epsilon)\sqrt{\frac{n}{d}}\left\{2\sqrt{n} + 5\sqrt{\frac{\log(n)}{\log(1+\epsilon)}}\right\} \\
 &= \oo\p{\frac{n}{\sqrt{d}}}.
\end{align}
Then, a form of Talagrand's inequality implies that
$\bP[\|A-\A\|_{\op} \geq \bE[\|A-\A\|_{\op}] + t] \leq \exp\p{-c\p{td/n}^2}$. 
Taking $t = (n/d)\sqrt{c^{-1}\log(1/\delta)}$, we have that 
\[ \bP\left[\|A-\A\|_{\op} \geq \oo\p{\frac{n}{\sqrt{d}} + \frac{n}{d}\sqrt{\log(1/\delta)}}\right] \leq \delta, \]
as was to be shown.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:subgradient}]
First, we can take $Z_0 = A - \mu UV^{\top}$, where $U \Lambda V^{\top}$ is the singular 
value decomposition of $M_0$. By assumption, $M_0 = PM_0$, and so $U = PU$ as well. Therefore, 
we do have $(UV^{\top})_{\sC} = \bi v_0^{\top}$ for some vector $v_0$. It remains to show that 
$\|v_0\|_2^2 \leq \frac{1}{|\sC|}$.

To see this, observe that $(UV^{\top})_{\sC}$ is a projection of $UV^{\top}$, and hence 
has operator norm at most $1$. However, because $(UV)^{\top}_{\sC}$ has rank $1$, its 
operator norm and Frobenius norm are equal, and so $\|\bi v_0^{\top}\|_F^2 \leq 1$. 
But $\|\bi v_0^{\top}\|_F^2 = \|\bi\|_2^2\|v_0\|_2^2 = |\sC|\|v_0\|_2^2$, from which 
the result follows.
\end{proof}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
