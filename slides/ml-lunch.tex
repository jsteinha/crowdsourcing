\documentclass{beamer}


%%%%%    \usetheme{CambridgeUS} % My favorite!
%%%%%    %\usetheme{Boadilla} % Pretty neat, soft color.
%%%%%    %\usetheme{default}
%%%%%    %\usetheme{Warsaw}
%%%%%    %\usetheme{Bergen} % This template has nagivation on the left
%%%%%    %\usetheme{Frankfurt} % Similar to the default 
%%%%%    %with an extra region at the top.
%%%%%    %\usecolortheme{seahorse} % Simple and clean template
%%%%%    %\usetheme{Darmstadt} % not so good
%%%%%    % Uncomment the following line if you want %
%%%%%    % page numbers and using Warsaw theme%
%%%%%    % \setbeamertemplate{footline}[page number]
%%%%%    %\setbeamercovered{transparent}
%%%%%    \setbeamercovered{invisible}
%%%%%    % To remove the navigation symbols from 
%%%%%    % the bottom of slides%
%%%%%    \setbeamertemplate{navigation symbols}{} 
%%%%%    %


\usepackage{beamerthemesplit}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{mathptmx}           % replacement for obsolete \usepackage{times}
\usepackage{algorithmic}
\usepackage[scaled=.90]{helvet} % replacement for obsolete \usepackage{times}
\usepackage{courier}            % replacement for obsolete \usepackage{times}

% set up Beamer style with Stanford colors and logo
% logo is available at http://nlp.stanford.edu/local/nlp-logos/nlp-logo.pdf
\useinnertheme{rounded}
\useoutertheme{infolines}
\usecolortheme{beaver}
\setbeamercolor{block title}{fg=white,bg=darkred!75!black}
\setbeamercolor{block body}{parent=normal text,bg=black!5!bg}
\setbeamercolor{item projected}{bg=darkred}
%\logo{\includegraphics[height=1cm]{nlp-logo.pdf}}

\DeclareMathOperator{\Geometric}{Geometric}

\newcommand{\utextbf}[1]{\textbf{\underline{#1}.}}
\newcommand{\point}[1]{\begin{itemize}\item[$\implies$\hskip -0.06in] #1 \end{itemize}}
\newcommand{\x}{\phantom{x}}
%\newcommand{\x}{x}
\newcommand{\s}{$\star$}

\usepackage{graphicx}
%\usepackage{bm}         % For typesetting bold math (not \mathbold)
%\logo{\includegraphics[height=0.6cm]{yourlogo.eps}}
%
\usepackage{import, subfiles}
\input{defs-beamer.tex}
\newcommand{\oo}{\mathcal{O}}
\newcommand{\p}[1]{\left(#1\right)}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\title[Community Detection with Adversaries]{Citizens, Dupes, and Frauds: Community Detection with Adversaries}
\author[Steinhardt, Valiant, \& Charikar]{Jacob Steinhardt \quad Gregory Valiant \quad Moses Charikar}
\institute[Stanford]
{
Stanford University \\
%\medskip
%{\emph{\{jsteinhardt,pliang\}@cs.stanford.edu}}
}
\date{\today}
% \today will show current date. 
% Alternatively, you can specify a date.
%
\begin{document}
%
\begin{frame}
\titlepage
\end{frame}
%
\begin{frame}
\frametitle{Setting}
\begin{itemize}
\item $N$ people
\item Each person $i$ produces actions $A_i$
\item Person $i$ can judge actions $A_j$ and produce a judgment $M_{i,j} \in \{0,1\}$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Examples}
\begin{itemize}
\item Crowdsourced data collection
\begin{itemize}
\item $A_i$ = data generated by turker $i$
\item $M_{i,j}$ = turker $i$'s rating of data from turker $j$
\end{itemize}
\pause
\item Twitter
\begin{itemize}
\item $A_i$ = tweets from user $i$
\item $M_{i,j}$ = whether $i$ follows $j$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Assumptions}
\begin{itemize}
\item $\alpha N$ people are \textbf{good}.
\begin{itemize}
\item produce good actions
\item make accurate judgments
\end{itemize}
\pause
\item Rest of people: no assumptions
\begin{itemize}
\item could be incompetent or even adversarial
\end{itemize}
\pause
\item We can also accurately judge people (but want to minimize our work).
\end{itemize}
\pause
\vskip 0.2in
Three groups of people:
\begin{itemize}
\item ``good'': good actions, good judgments
\item ``imposers'': good actions, bad judgments
\item ``bad'': bad actions
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Result}
Goal: find the good people (or at least people with good actions).
\pause
\vskip 0.2in
Ideally, each individual only needs to make a small number of judgments.
\pause
\vskip 0.2in
{\bf Main result:} if each person judges $\oo\p{\frac{\log(1/\alpha)}{\alpha^2}}$ other people 
at random, we can mostly recover the good people w.h.p.
\pause
\begin{itemize}
\item Also need $\oo\p{\frac{\log(1/\alpha)}{\alpha^2}}$ judgments of our own.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{What does recovery mean?}
Find a set $S$ of people such that:
\begin{itemize}
\item Most of the actions in $S$ are good.
\item $|S| \geq \frac{\alpha}{2}N$.
\end{itemize}
\pause
E.g. get mostly good data from turkers, identify many users with good tweets.
\pause
\vskip 0.2in
Note: ideally can expand $S$ via majority vote, but not sure how to 
make this work in presence of imposters.
\end{frame}

\begin{frame}
\frametitle{Algorithm (first attempt)}
Assume good people are \textbf{perfect}: $M_{i,j} = 1$ iff $A_j$ is good.
\pause
\vskip 0.2in
Algorithm:
\begin{enumerate}
\item Find maximum size set $S_0$ such that all ratings inside $S_0$ are positive.
\pause
\item Judge a random subset of $S_0$ of size $\oo\p{\frac{\log(1/\alpha)}{\alpha}}$.
\pause
\item If subset is good, accept $S_0$ as $S$.
\pause
\item If subset is bad, remove $S_0$ and start over.
\end{enumerate}
\pause
\vskip 0.2in
Key lemma: if $S_0$ contains $\geq \frac{\alpha}{8}N$ bad actions, contains 
$\leq \frac{\alpha^2}{4}N$ good people.
\pause
\vskip 0.01in
$\implies$ At most $\frac{2}{\alpha}$ rounds of algorithm, $\leq \frac{\alpha}{2}N$ good people removed in total.
\end{frame}

\begin{frame}
\frametitle{Proof of Key Lemma}
Suppose there are $\geq \frac{\alpha}{8}N$ bad actions, $\geq \frac{\alpha^2}{4}N$ good people in $S_0$.
\vskip 0.2in
None can rate each other, otherwise would have negative rating.
\vskip 0.2in
Probability of no ratings: $\p{1-\frac{c\log(1/\alpha)}{\alpha^2N}}^{\frac{\alpha^3N^2}{32}} \approx \exp\p{-\frac{c}{32}\alpha\log(1/\alpha) N}$.
\vskip 0.2in
Number of possible pairs of sets: $\binom{N}{\frac{\alpha}{8}N}\binom{N}{\frac{\alpha^2}{4}N} = \exp\p{\oo\p{\alpha\log(1/\alpha)N}}$.
\vskip 0.2in
For sufficiently large $c$, w.h.p. all such sets have at least one rating.
\end{frame}

\begin{frame}
\frametitle{Takeaways}
Don't need to fully solve a problem automatically, just need to 
reduce to small amount of interaction with user.
\pause
\vskip 0.2in
Issues:
\begin{itemize}
\item max clique = NP-hard
\item perfect judgments: not very plausible
\end{itemize}
Address both of these in rest of talk.
\end{frame}

\begin{frame}
\frametitle{An Efficient Algorithm}
Consider full matrix $M$. Let $\sC \eqdef$ set of good people.
\pause
\vskip 0.2in
Properties:
\begin{enumerate}
\item $M_{i,j} = M_{i',j}$ if $i,i' \in \sC$.
\item $M_{i,j} = 1$ if $i,j \in \sC$.
\end{enumerate}
\pause
\vskip 0.2in
Algorithm:
\begin{itemize}
\item Take any $i_0 \in \sC$.
\item Let $S_0 = \{j \mid M_{i_0,j} = 1\}$.
\item Let $S_1 = \{i \in S_0 \mid M_{i,j} = 1\text{ for all }j \in S_0\}$.
\end{itemize}
\pause
$S_1$ is a clique containing $\sC$.
\end{frame}

\begin{frame}
\frametitle{Recovering $M$}
Want to recover $M$ in a noise-robust way. Use 
low-rank matrix completion:
\[ \max_{Y} \langle Y, M \rangle - \frac{\alpha N}{4}\|Y\|_* \text{ s.t. } 0 \leq Y_{i,j} \leq 1. \]
\pause
Key properties of optimum $Y^*$:
\begin{itemize}
\item $Y^*_{i,j} = Y^*_{i',j}$ if $i,i' \in \sC$.
\item $Y^*_{i,j} = 1$ for all $i,j \in \sC_0$, where $\sC_0 \subseteq \sC$ has size $\geq \frac{3\alpha}{4}N$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Proof of 1}
Consider projection matrix $P$:
\[ P_{i,j} = \left\{ \begin{array}{ccl} \frac{1}{|\sC|} & : & i,j \in \sC \\ \delta_{i,j} & : & \text{else} \end{array} \right. \]
\pause
First, $\langle PY, M \rangle = \langle Y, PM \rangle = \langle Y, M \rangle$.
\pause \vskip 0.1in
Second, $\|PY\|_* \leq \|Y\|_*$.
\pause \vskip 0.2in
Therefore, $PY$ has higher objective value than $Y$ $\implies PY^* = Y^*$.
\end{frame}

\begin{frame}
\frametitle{Final step: noise-robustness}
Only actually see $M_{i,j}$ with probability $p \approx \frac{\log(1/\alpha)}{\alpha^2N}$.
\pause \vskip 0.2in
Let $\tilde{M}$ be defined as
\[ \tilde{M}_{i,j} = \left\{ \begin{array}{ccl} \frac{1}{p}M_{i,j} & : & M_{i,j} \text{ is observed} \\ 0 & : & \text{else} \end{array} \right. \]
\pause
Intuition:
\begin{itemize}
\item $\tilde{M}$ is unbiased estimate of $M$
\item nuclear norm yields noise-robustness
\end{itemize}
\pause
Hard part: show that $PY^* = Y^*$ still holds.
\end{frame}

\newcommand{\op}{\mathrm{op}}
\begin{frame}
\frametitle{Attempted proof (help!)}
Need to show:
\[ \langle Y, \tilde{M} \rangle - \frac{\alpha N}{4}\|Y\|_* \leq \langle PY, \tilde{M} \rangle - \frac{\alpha N}{4} \|PY\|_* \]
\pause
Re-arranging:
\[ \langle (I-P)Y, \tilde{M} \rangle \leq \frac{\alpha N}{4}\p{\|Y\|_* - \|PY\|_*} \]
\pause
Note:
\vskip -0.35in
\begin{align*}
\langle (I-P)Y, \tilde{M} \rangle &= \langle (I-P)Y, \tilde{M} - M \rangle \\
 &\leq \|(I-P)Y\|_*\|\tilde{M} - M\|_{\op} \\
 &\leq \oo\p{\sqrt{N/p}}\|(I-P)Y\|_*
\end{align*}
So want $c \cdot \sqrt{N/p}\|(I-P)\|_* \leq (\alpha N/4)\p{\|Y\|_* - \|PY\|_*}$. (Almost true$\ldots$)
\end{frame}

\begin{frame}
\frametitle{Fin.}
Thanks!
\end{frame}

\end{document} 
